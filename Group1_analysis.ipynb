{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "176359e2-8e14-42b3-9d12-091fffa089e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+----+---------+--------+----------+-------+---------------+--------+-------+\n|FlightDate|Origin|Dest|Cancelled|Diverted|CRSDepTime|DepTime|DepDelayMinutes|DepDelay|ArrTime|\n+----------+------+----+---------+--------+----------+-------+---------------+--------+-------+\n|2018-03-24|   LIH| HNL|    false|   false|      1900| 1855.0|            0.0|    -5.0| 1927.0|\n|2018-04-23|   BOS| PIT|    false|   false|      2055| 2111.0|           16.0|    16.0| 2316.0|\n|2018-02-16|   HOU| ECP|    false|   false|      1650| 1728.0|           38.0|    38.0| 1903.0|\n|2018-12-07|   MDW| MCO|    false|   false|      1845| 1851.0|            6.0|     6.0| 2214.0|\n|2018-06-03|   LAS| LIT|    false|   false|      1200| 1201.0|            1.0|     1.0| 1639.0|\n+----------+------+----+---------+--------+----------+-------+---------------+--------+-------+\nonly showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "columns = df.columns[:10]  # Get the first 10 column names\n",
    "df_selected = df.select(*columns)\n",
    "\n",
    "# Show data\n",
    "df_selected.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfc3e3a9-6715-4d2e-802f-4209b10dae87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------+--------------+-----------------+--------+----+-------+-----+----------+---------+\n|ArrDelayMinutes|AirTime|CRSElapsedTime|ActualElapsedTime|Distance|Year|Quarter|Month|DayofMonth|DayOfWeek|\n+---------------+-------+--------------+-----------------+--------+----+-------+-----+----------+---------+\n|            0.0|   19.0|          35.0|             32.0|   102.0|2018|      1|    3|        24|        6|\n|           27.0|   80.0|         114.0|            125.0|   496.0|2018|      2|    4|        23|        1|\n|           43.0|   75.0|          90.0|             95.0|   571.0|2018|      1|    2|        16|        5|\n|            0.0|  127.0|         150.0|            143.0|   990.0|2018|      4|   12|         7|        5|\n|            0.0|  149.0|         175.0|            158.0|  1294.0|2018|      2|    6|         3|        7|\n+---------------+-------+--------------+-----------------+--------+----+-------+-----+----------+---------+\nonly showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Select columns 11-20\n",
    "columns_11_20 = df.columns[10:20]\n",
    "df_11_20 = df.select(*columns_11_20)\n",
    "\n",
    "# Show data\n",
    "df_11_20.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed692f62-2403-4969-8d90-a8ea82517df1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---------------+--------------+---------------+-------------+\n|Operating_Airline|OriginAirportID|OriginCityName|OriginStateName|DestAirportID|\n+-----------------+---------------+--------------+---------------+-------------+\n|               HA|          12982|         Lihue|         Hawaii|        12173|\n|               9E|          10721|        Boston|  Massachusetts|        14122|\n|               WN|          12191|       Houston|          Texas|        11481|\n|               WN|          13232|       Chicago|       Illinois|        13204|\n|               WN|          12889|     Las Vegas|         Nevada|        12992|\n+-----------------+---------------+--------------+---------------+-------------+\nonly showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Select columns 21-30\n",
    "columns_21_30 = df.columns[20:25]\n",
    "df_21_30 = df.select(*columns_21_30)\n",
    "\n",
    "# Show data\n",
    "df_21_30.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e38893c0-a80d-4646-a6ce-70678da2ffd4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+-------------+--------+--------------------+\n|DestCityName|DestState|DestStateName|DepDel15|DepartureDelayGroups|\n+------------+---------+-------------+--------+--------------------+\n|    Honolulu|       HI|       Hawaii|     0.0|                -1.0|\n|  Pittsburgh|       PA| Pennsylvania|     1.0|                 1.0|\n| Panama City|       FL|      Florida|     1.0|                 2.0|\n|     Orlando|       FL|      Florida|     0.0|                 0.0|\n| Little Rock|       AR|     Arkansas|     0.0|                 0.0|\n+------------+---------+-------------+--------+--------------------+\nonly showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Select columns 21-30\n",
    "columns_21_30 = df.columns[25:30]\n",
    "df_21_30 = df.select(*columns_21_30)\n",
    "\n",
    "# Show data\n",
    "df_21_30.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48b269b5-c53a-4f43-a5ad-a0a3d70be934",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+--------+------+----------+--------+--------+------------------+\n|TaxiOut|WheelsOff|WheelsOn|TaxiIn|CRSArrTime|ArrDelay|ArrDel15|ArrivalDelayGroups|\n+-------+---------+--------+------+----------+--------+--------+------------------+\n|    6.0|   1901.0|  1920.0|   7.0|      1935|    -8.0|     0.0|              -1.0|\n|   22.0|   2133.0|  2253.0|  23.0|      2249|    27.0|     1.0|               1.0|\n|   16.0|   1744.0|  1859.0|   4.0|      1820|    43.0|     1.0|               2.0|\n|    9.0|   1900.0|  2207.0|   7.0|      2215|    -1.0|     0.0|              -1.0|\n|    7.0|   1208.0|  1637.0|   2.0|      1655|   -16.0|     0.0|              -2.0|\n+-------+---------+--------+------+----------+--------+--------+------------------+\nonly showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Select columns 31-40\n",
    "columns_31_40 = df.columns[30:38]\n",
    "df_31_40 = df.select(*columns_31_40)\n",
    "\n",
    "# Show data\n",
    "df_31_40.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0de61401-6981-4a31-ab1e-03dbf3f645e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------+-------------------+\n|DistanceGroup|Delayed|        Airline_enc|\n+-------------+-------+-------------------+\n|            1|      0|        0.111328125|\n|            2|      1| 0.1278928136419001|\n|            3|      1|0.17506956037840846|\n|            4|      0|0.17506956037840846|\n|            6|      0|0.17506956037840846|\n+-------------+-------+-------------------+\nonly showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Select columns 31-40\n",
    "columns_31_40 = df.columns[38:]\n",
    "df_31_40 = df.select(*columns_31_40)\n",
    "\n",
    "# Show data\n",
    "df_31_40.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f89dc643-20ee-49c7-b7db-9d29f3c28fc7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2dea064c-a9be-435d-89b5-c33704fdb921",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-5658781754548474>, line 9\u001B[0m\n",
       "\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m# Plot the count plot\u001B[39;00m\n",
       "\u001B[1;32m      8\u001B[0m plt\u001B[38;5;241m.\u001B[39mfigure(figsize\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m10\u001B[39m, \u001B[38;5;241m6\u001B[39m))\n",
       "\u001B[0;32m----> 9\u001B[0m sns\u001B[38;5;241m.\u001B[39mcountplot(x\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDelayed\u001B[39m\u001B[38;5;124m\"\u001B[39m, hue\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAirline\u001B[39m\u001B[38;5;124m\"\u001B[39m, data\u001B[38;5;241m=\u001B[39mdf_pandas, palette\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mviridis\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m     10\u001B[0m plt\u001B[38;5;241m.\u001B[39mtitle(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFlight Delays by Airline\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m     11\u001B[0m plt\u001B[38;5;241m.\u001B[39mxlabel(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDelayed (0 = No, 1 = Yes)\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/seaborn/categorical.py:2631\u001B[0m, in \u001B[0;36mcountplot\u001B[0;34m(data, x, y, hue, order, hue_order, orient, color, palette, saturation, fill, hue_norm, stat, width, dodge, gap, log_scale, native_scale, formatter, legend, ax, **kwargs)\u001B[0m\n",
       "\u001B[1;32m   2628\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m x \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m y \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
       "\u001B[1;32m   2629\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot pass values for both `x` and `y`.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[0;32m-> 2631\u001B[0m p \u001B[38;5;241m=\u001B[39m _CategoricalAggPlotter(\n",
       "\u001B[1;32m   2632\u001B[0m     data\u001B[38;5;241m=\u001B[39mdata,\n",
       "\u001B[1;32m   2633\u001B[0m     variables\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mdict\u001B[39m(x\u001B[38;5;241m=\u001B[39mx, y\u001B[38;5;241m=\u001B[39my, hue\u001B[38;5;241m=\u001B[39mhue),\n",
       "\u001B[1;32m   2634\u001B[0m     order\u001B[38;5;241m=\u001B[39morder,\n",
       "\u001B[1;32m   2635\u001B[0m     orient\u001B[38;5;241m=\u001B[39morient,\n",
       "\u001B[1;32m   2636\u001B[0m     color\u001B[38;5;241m=\u001B[39mcolor,\n",
       "\u001B[1;32m   2637\u001B[0m     legend\u001B[38;5;241m=\u001B[39mlegend,\n",
       "\u001B[1;32m   2638\u001B[0m )\n",
       "\u001B[1;32m   2640\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m ax \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
       "\u001B[1;32m   2641\u001B[0m     ax \u001B[38;5;241m=\u001B[39m plt\u001B[38;5;241m.\u001B[39mgca()\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/seaborn/categorical.py:67\u001B[0m, in \u001B[0;36m_CategoricalPlotter.__init__\u001B[0;34m(self, data, variables, order, orient, require_numeric, color, legend)\u001B[0m\n",
       "\u001B[1;32m     56\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\n",
       "\u001B[1;32m     57\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n",
       "\u001B[1;32m     58\u001B[0m     data\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m     64\u001B[0m     legend\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mauto\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m     65\u001B[0m ):\n",
       "\u001B[0;32m---> 67\u001B[0m     \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(data\u001B[38;5;241m=\u001B[39mdata, variables\u001B[38;5;241m=\u001B[39mvariables)\n",
       "\u001B[1;32m     69\u001B[0m     \u001B[38;5;66;03m# This method takes care of some bookkeeping that is necessary because the\u001B[39;00m\n",
       "\u001B[1;32m     70\u001B[0m     \u001B[38;5;66;03m# original categorical plots (prior to the 2021 refactor) had some rules that\u001B[39;00m\n",
       "\u001B[1;32m     71\u001B[0m     \u001B[38;5;66;03m# don't fit exactly into VectorPlotter logic. It may be wise to have a second\u001B[39;00m\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m     76\u001B[0m     \u001B[38;5;66;03m# default VectorPlotter rules. If we do decide to make orient part of the\u001B[39;00m\n",
       "\u001B[1;32m     77\u001B[0m     \u001B[38;5;66;03m# _base variable assignment, we'll want to figure out how to express that.\u001B[39;00m\n",
       "\u001B[1;32m     78\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minput_format \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwide\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m orient \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mh\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124my\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/seaborn/_base.py:634\u001B[0m, in \u001B[0;36mVectorPlotter.__init__\u001B[0;34m(self, data, variables)\u001B[0m\n",
       "\u001B[1;32m    629\u001B[0m \u001B[38;5;66;03m# var_ordered is relevant only for categorical axis variables, and may\u001B[39;00m\n",
       "\u001B[1;32m    630\u001B[0m \u001B[38;5;66;03m# be better handled by an internal axis information object that tracks\u001B[39;00m\n",
       "\u001B[1;32m    631\u001B[0m \u001B[38;5;66;03m# such information and is set up by the scale_* methods. The analogous\u001B[39;00m\n",
       "\u001B[1;32m    632\u001B[0m \u001B[38;5;66;03m# information for numeric axes would be information about log scales.\u001B[39;00m\n",
       "\u001B[1;32m    633\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_var_ordered \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mx\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28;01mFalse\u001B[39;00m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124my\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28;01mFalse\u001B[39;00m}  \u001B[38;5;66;03m# alt., used DefaultDict\u001B[39;00m\n",
       "\u001B[0;32m--> 634\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39massign_variables(data, variables)\n",
       "\u001B[1;32m    636\u001B[0m \u001B[38;5;66;03m# TODO Lots of tests assume that these are called to initialize the\u001B[39;00m\n",
       "\u001B[1;32m    637\u001B[0m \u001B[38;5;66;03m# mappings to default values on class initialization. I'd prefer to\u001B[39;00m\n",
       "\u001B[1;32m    638\u001B[0m \u001B[38;5;66;03m# move away from that and only have a mapping when explicitly called.\u001B[39;00m\n",
       "\u001B[1;32m    639\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m var \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhue\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msize\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstyle\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/seaborn/_base.py:679\u001B[0m, in \u001B[0;36mVectorPlotter.assign_variables\u001B[0;34m(self, data, variables)\u001B[0m\n",
       "\u001B[1;32m    674\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    675\u001B[0m     \u001B[38;5;66;03m# When dealing with long-form input, use the newer PlotData\u001B[39;00m\n",
       "\u001B[1;32m    676\u001B[0m     \u001B[38;5;66;03m# object (internal but introduced for the objects interface)\u001B[39;00m\n",
       "\u001B[1;32m    677\u001B[0m     \u001B[38;5;66;03m# to centralize / standardize data consumption logic.\u001B[39;00m\n",
       "\u001B[1;32m    678\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minput_format \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlong\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[0;32m--> 679\u001B[0m     plot_data \u001B[38;5;241m=\u001B[39m PlotData(data, variables)\n",
       "\u001B[1;32m    680\u001B[0m     frame \u001B[38;5;241m=\u001B[39m plot_data\u001B[38;5;241m.\u001B[39mframe\n",
       "\u001B[1;32m    681\u001B[0m     names \u001B[38;5;241m=\u001B[39m plot_data\u001B[38;5;241m.\u001B[39mnames\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/seaborn/_core/data.py:58\u001B[0m, in \u001B[0;36mPlotData.__init__\u001B[0;34m(self, data, variables)\u001B[0m\n",
       "\u001B[1;32m     51\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n",
       "\u001B[1;32m     53\u001B[0m     data: DataSource,\n",
       "\u001B[1;32m     54\u001B[0m     variables: \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, VariableSpec],\n",
       "\u001B[1;32m     55\u001B[0m ):\n",
       "\u001B[1;32m     57\u001B[0m     data \u001B[38;5;241m=\u001B[39m handle_data_source(data)\n",
       "\u001B[0;32m---> 58\u001B[0m     frame, names, ids \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_assign_variables(data, variables)\n",
       "\u001B[1;32m     60\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mframe \u001B[38;5;241m=\u001B[39m frame\n",
       "\u001B[1;32m     61\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnames \u001B[38;5;241m=\u001B[39m names\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/seaborn/_core/data.py:232\u001B[0m, in \u001B[0;36mPlotData._assign_variables\u001B[0;34m(self, data, variables)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    231\u001B[0m         err \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn entry with this name does not appear in `data`.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[0;32m--> 232\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(err)\n",
       "\u001B[1;32m    234\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    235\u001B[0m \n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;66;03m# Otherwise, assume the value somehow represents data\u001B[39;00m\n",
       "\u001B[1;32m    237\u001B[0m \n",
       "\u001B[1;32m    238\u001B[0m     \u001B[38;5;66;03m# Ignore empty data structures\u001B[39;00m\n",
       "\u001B[1;32m    239\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(val, Sized) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(val) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
       "\n",
       "\u001B[0;31mValueError\u001B[0m: Could not interpret value `Airline` for `hue`. An entry with this name does not appear in `data`."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "ValueError",
        "evalue": "Could not interpret value `Airline` for `hue`. An entry with this name does not appear in `data`."
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>ValueError</span>: Could not interpret value `Airline` for `hue`. An entry with this name does not appear in `data`."
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
        "File \u001B[0;32m<command-5658781754548474>, line 9\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m# Plot the count plot\u001B[39;00m\n\u001B[1;32m      8\u001B[0m plt\u001B[38;5;241m.\u001B[39mfigure(figsize\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m10\u001B[39m, \u001B[38;5;241m6\u001B[39m))\n\u001B[0;32m----> 9\u001B[0m sns\u001B[38;5;241m.\u001B[39mcountplot(x\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDelayed\u001B[39m\u001B[38;5;124m\"\u001B[39m, hue\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAirline\u001B[39m\u001B[38;5;124m\"\u001B[39m, data\u001B[38;5;241m=\u001B[39mdf_pandas, palette\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mviridis\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     10\u001B[0m plt\u001B[38;5;241m.\u001B[39mtitle(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFlight Delays by Airline\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     11\u001B[0m plt\u001B[38;5;241m.\u001B[39mxlabel(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDelayed (0 = No, 1 = Yes)\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/seaborn/categorical.py:2631\u001B[0m, in \u001B[0;36mcountplot\u001B[0;34m(data, x, y, hue, order, hue_order, orient, color, palette, saturation, fill, hue_norm, stat, width, dodge, gap, log_scale, native_scale, formatter, legend, ax, **kwargs)\u001B[0m\n\u001B[1;32m   2628\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m x \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m y \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   2629\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot pass values for both `x` and `y`.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m-> 2631\u001B[0m p \u001B[38;5;241m=\u001B[39m _CategoricalAggPlotter(\n\u001B[1;32m   2632\u001B[0m     data\u001B[38;5;241m=\u001B[39mdata,\n\u001B[1;32m   2633\u001B[0m     variables\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mdict\u001B[39m(x\u001B[38;5;241m=\u001B[39mx, y\u001B[38;5;241m=\u001B[39my, hue\u001B[38;5;241m=\u001B[39mhue),\n\u001B[1;32m   2634\u001B[0m     order\u001B[38;5;241m=\u001B[39morder,\n\u001B[1;32m   2635\u001B[0m     orient\u001B[38;5;241m=\u001B[39morient,\n\u001B[1;32m   2636\u001B[0m     color\u001B[38;5;241m=\u001B[39mcolor,\n\u001B[1;32m   2637\u001B[0m     legend\u001B[38;5;241m=\u001B[39mlegend,\n\u001B[1;32m   2638\u001B[0m )\n\u001B[1;32m   2640\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m ax \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   2641\u001B[0m     ax \u001B[38;5;241m=\u001B[39m plt\u001B[38;5;241m.\u001B[39mgca()\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/seaborn/categorical.py:67\u001B[0m, in \u001B[0;36m_CategoricalPlotter.__init__\u001B[0;34m(self, data, variables, order, orient, require_numeric, color, legend)\u001B[0m\n\u001B[1;32m     56\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\n\u001B[1;32m     57\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m     58\u001B[0m     data\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     64\u001B[0m     legend\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mauto\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     65\u001B[0m ):\n\u001B[0;32m---> 67\u001B[0m     \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(data\u001B[38;5;241m=\u001B[39mdata, variables\u001B[38;5;241m=\u001B[39mvariables)\n\u001B[1;32m     69\u001B[0m     \u001B[38;5;66;03m# This method takes care of some bookkeeping that is necessary because the\u001B[39;00m\n\u001B[1;32m     70\u001B[0m     \u001B[38;5;66;03m# original categorical plots (prior to the 2021 refactor) had some rules that\u001B[39;00m\n\u001B[1;32m     71\u001B[0m     \u001B[38;5;66;03m# don't fit exactly into VectorPlotter logic. It may be wise to have a second\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     76\u001B[0m     \u001B[38;5;66;03m# default VectorPlotter rules. If we do decide to make orient part of the\u001B[39;00m\n\u001B[1;32m     77\u001B[0m     \u001B[38;5;66;03m# _base variable assignment, we'll want to figure out how to express that.\u001B[39;00m\n\u001B[1;32m     78\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minput_format \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwide\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m orient \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mh\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124my\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/seaborn/_base.py:634\u001B[0m, in \u001B[0;36mVectorPlotter.__init__\u001B[0;34m(self, data, variables)\u001B[0m\n\u001B[1;32m    629\u001B[0m \u001B[38;5;66;03m# var_ordered is relevant only for categorical axis variables, and may\u001B[39;00m\n\u001B[1;32m    630\u001B[0m \u001B[38;5;66;03m# be better handled by an internal axis information object that tracks\u001B[39;00m\n\u001B[1;32m    631\u001B[0m \u001B[38;5;66;03m# such information and is set up by the scale_* methods. The analogous\u001B[39;00m\n\u001B[1;32m    632\u001B[0m \u001B[38;5;66;03m# information for numeric axes would be information about log scales.\u001B[39;00m\n\u001B[1;32m    633\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_var_ordered \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mx\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28;01mFalse\u001B[39;00m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124my\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28;01mFalse\u001B[39;00m}  \u001B[38;5;66;03m# alt., used DefaultDict\u001B[39;00m\n\u001B[0;32m--> 634\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39massign_variables(data, variables)\n\u001B[1;32m    636\u001B[0m \u001B[38;5;66;03m# TODO Lots of tests assume that these are called to initialize the\u001B[39;00m\n\u001B[1;32m    637\u001B[0m \u001B[38;5;66;03m# mappings to default values on class initialization. I'd prefer to\u001B[39;00m\n\u001B[1;32m    638\u001B[0m \u001B[38;5;66;03m# move away from that and only have a mapping when explicitly called.\u001B[39;00m\n\u001B[1;32m    639\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m var \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhue\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msize\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstyle\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/seaborn/_base.py:679\u001B[0m, in \u001B[0;36mVectorPlotter.assign_variables\u001B[0;34m(self, data, variables)\u001B[0m\n\u001B[1;32m    674\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    675\u001B[0m     \u001B[38;5;66;03m# When dealing with long-form input, use the newer PlotData\u001B[39;00m\n\u001B[1;32m    676\u001B[0m     \u001B[38;5;66;03m# object (internal but introduced for the objects interface)\u001B[39;00m\n\u001B[1;32m    677\u001B[0m     \u001B[38;5;66;03m# to centralize / standardize data consumption logic.\u001B[39;00m\n\u001B[1;32m    678\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minput_format \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlong\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m--> 679\u001B[0m     plot_data \u001B[38;5;241m=\u001B[39m PlotData(data, variables)\n\u001B[1;32m    680\u001B[0m     frame \u001B[38;5;241m=\u001B[39m plot_data\u001B[38;5;241m.\u001B[39mframe\n\u001B[1;32m    681\u001B[0m     names \u001B[38;5;241m=\u001B[39m plot_data\u001B[38;5;241m.\u001B[39mnames\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/seaborn/_core/data.py:58\u001B[0m, in \u001B[0;36mPlotData.__init__\u001B[0;34m(self, data, variables)\u001B[0m\n\u001B[1;32m     51\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m     53\u001B[0m     data: DataSource,\n\u001B[1;32m     54\u001B[0m     variables: \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, VariableSpec],\n\u001B[1;32m     55\u001B[0m ):\n\u001B[1;32m     57\u001B[0m     data \u001B[38;5;241m=\u001B[39m handle_data_source(data)\n\u001B[0;32m---> 58\u001B[0m     frame, names, ids \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_assign_variables(data, variables)\n\u001B[1;32m     60\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mframe \u001B[38;5;241m=\u001B[39m frame\n\u001B[1;32m     61\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnames \u001B[38;5;241m=\u001B[39m names\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/seaborn/_core/data.py:232\u001B[0m, in \u001B[0;36mPlotData._assign_variables\u001B[0;34m(self, data, variables)\u001B[0m\n\u001B[1;32m    230\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    231\u001B[0m         err \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn entry with this name does not appear in `data`.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m--> 232\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(err)\n\u001B[1;32m    234\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    235\u001B[0m \n\u001B[1;32m    236\u001B[0m     \u001B[38;5;66;03m# Otherwise, assume the value somehow represents data\u001B[39;00m\n\u001B[1;32m    237\u001B[0m \n\u001B[1;32m    238\u001B[0m     \u001B[38;5;66;03m# Ignore empty data structures\u001B[39;00m\n\u001B[1;32m    239\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(val, Sized) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(val) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
        "\u001B[0;31mValueError\u001B[0m: Could not interpret value `Airline` for `hue`. An entry with this name does not appear in `data`."
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert PySpark DataFrame to Pandas\n",
    "df_pandas = df.toPandas()\n",
    "\n",
    "# Plot the count plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x=\"Delayed\", hue=\"Airline\", data=df_pandas, palette=\"viridis\")\n",
    "plt.title(\"Flight Delays by Airline\")\n",
    "plt.xlabel(\"Delayed (0 = No, 1 = Yes)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.legend(title=\"Airline\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a1be119-395e-4af1-93c5-cafe4ec76023",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"ts\": \"2025-02-04 13:10:29,975\", \"level\": \"ERROR\", \"logger\": \"DataFrameQueryContextLogger\", \"msg\": \"[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `Airline` cannot be resolved. Did you mean one of the following? [`AirTime`, `ArrTime`, `Origin`, `Airline_enc`, `TaxiIn`]. SQLSTATE: 42703\", \"context\": {\"file\": \"jdk.internal.reflect.GeneratedMethodAccessor344.invoke(Unknown Source)\", \"line\": \"\", \"fragment\": \"col\", \"errorClass\": \"UNRESOLVED_COLUMN.WITH_SUGGESTION\"}, \"exception\": {\"class\": \"Py4JJavaError\", \"msg\": \"An error occurred while calling o22035.agg.\\n: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `Airline` cannot be resolved. Did you mean one of the following? [`AirTime`, `ArrTime`, `Origin`, `Airline_enc`, `TaxiIn`]. SQLSTATE: 42703;\\n'Aggregate ['Airline], ['Airline, count(1) AS TotalFlights#21073L, count(CASE WHEN (Delayed#2400 = 1) THEN true END) AS DelayedFlights#21074L]\\n+- Project [FlightDate#2360, Origin#2362, Dest#2363, Cancelled#2364, Diverted#2365, CRSDepTime#2366, DepTime#2367, DepDelayMinutes#2368, DepDelay#2369, ArrTime#2370, ArrDelayMinutes#2371, AirTime#2372, CRSElapsedTime#2373, ActualElapsedTime#2374, Distance#2375, Year#2376, Quarter#2377, Month#2378, DayofMonth#2379, DayOfWeek#2380, Operating_Airline#2381, OriginAirportID#2382, OriginCityName#2383, OriginStateName#2384, ... 17 more fields]\\n   +- Project [Airline#2361, FlightDate#2360, Origin#2362, Dest#2363, Cancelled#2364, Diverted#2365, CRSDepTime#2366, DepTime#2367, DepDelayMinutes#2368, DepDelay#2369, ArrTime#2370, ArrDelayMinutes#2371, AirTime#2372, CRSElapsedTime#2373, ActualElapsedTime#2374, Distance#2375, Year#2376, Quarter#2377, Month#2378, DayofMonth#2379, DayOfWeek#2380, Operating_Airline#2381, OriginAirportID#2382, OriginCityName#2383, ... 18 more fields]\\n      +- Join LeftOuter, (Airline#2361 = Airline#18448)\\n         :- Relation [FlightDate#2360,Airline#2361,Origin#2362,Dest#2363,Cancelled#2364,Diverted#2365,CRSDepTime#2366,DepTime#2367,DepDelayMinutes#2368,DepDelay#2369,ArrTime#2370,ArrDelayMinutes#2371,AirTime#2372,CRSElapsedTime#2373,ActualElapsedTime#2374,Distance#2375,Year#2376,Quarter#2377,Month#2378,DayofMonth#2379,DayOfWeek#2380,Operating_Airline#2381,OriginAirportID#2382,OriginCityName#2383,... 17 more fields] csv\\n         +- Aggregate [Airline#18448], [Airline#18448, avg(Delayed#18487) AS Airline_enc#18443]\\n            +- Relation [FlightDate#18447,Airline#18448,Origin#18449,Dest#18450,Cancelled#18451,Diverted#18452,CRSDepTime#18453,DepTime#18454,DepDelayMinutes#18455,DepDelay#18456,ArrTime#18457,ArrDelayMinutes#18458,AirTime#18459,CRSElapsedTime#18460,ActualElapsedTime#18461,Distance#18462,Year#18463,Quarter#18464,Month#18465,DayofMonth#18466,DayOfWeek#18467,Operating_Airline#18468,OriginAirportID#18469,OriginCityName#18470,... 17 more fields] csv\\n\\n\\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:481)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:181)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$8(CheckAnalysis.scala:421)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$8$adapted(CheckAnalysis.scala:406)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:287)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7(CheckAnalysis.scala:406)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7$adapted(CheckAnalysis.scala:406)\\n\\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\\n\\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\\n\\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:406)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:286)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:287)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:286)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:261)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:406)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:246)\\n\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\\n\\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:233)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:233)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:406)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:171)\\n\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\\n\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:201)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:171)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:68)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:94)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:63)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:473)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:443)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:473)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:276)\\n\\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\\n\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:483)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:593)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:145)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:593)\\n\\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1224)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:592)\\n\\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:588)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1384)\\n\\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:588)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:270)\\n\\tat scala.util.Try$.apply(Try.scala:213)\\n\\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1676)\\n\\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1737)\\n\\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\\n\\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:297)\\n\\tat org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfos(DatasourceAttributeExtractor.scala:88)\\n\\tat org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfos$(DatasourceAttributeExtractor.scala:85)\\n\\tat org.mlflow.spark.autologging.ReplAwareDatasourceAttributeExtractor$.getTableInfos(DatasourceAttributeExtractor.scala:142)\\n\\tat org.mlflow.spark.autologging.ReplAwareSparkDataSourceListener.onSQLExecutionEnd(ReplAwareSparkDataSourceListener.scala:49)\\n\\tat org.mlflow.spark.autologging.SparkDataSourceListener.$anonfun$onOtherEvent$1(SparkDataSourceListener.scala:39)\\n\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\\n\\tat org.mlflow.spark.autologging.ExceptionUtils$.tryAndLogUnexpectedError(ExceptionUtils.scala:26)\\n\\tat org.mlflow.spark.autologging.SparkDataSourceListener.onOtherEvent(SparkDataSourceListener.scala:39)\\n\\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:108)\\n\\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\\n\\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:42)\\n\\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:42)\\n\\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:199)\\n\\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:169)\\n\\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:116)\\n\\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:116)\\n\\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\\n\\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\\n\\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:111)\\n\\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:107)\\n\\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1564)\\n\\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:107)\\n\\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\\n\\t\\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:481)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:181)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$8(CheckAnalysis.scala:421)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$8$adapted(CheckAnalysis.scala:406)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:287)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7(CheckAnalysis.scala:406)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7$adapted(CheckAnalysis.scala:406)\\n\\t\\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\\n\\t\\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\\n\\t\\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:406)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:286)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:287)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:286)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:261)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:406)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:246)\\n\\t\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\\n\\t\\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:233)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:233)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:406)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:171)\\n\\t\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\\n\\t\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:201)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:171)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:68)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:94)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:63)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:473)\\n\\t\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:443)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:473)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:276)\\n\\t\\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\\n\\t\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:483)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:593)\\n\\t\\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:145)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:593)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1224)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:592)\\n\\t\\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:588)\\n\\t\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1384)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:588)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:270)\\n\\t\\tat scala.util.Try$.apply(Try.scala:213)\\n\\t\\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1676)\\n\\t\\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\\n\\t\\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\\n\\t\\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:297)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:251)\\n\\t\\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:106)\\n\\t\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1384)\\n\\t\\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1391)\\n\\t\\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\\n\\t\\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1391)\\n\\t\\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:104)\\n\\t\\tat org.apache.spark.sql.RelationalGroupedDataset.toDF(RelationalGroupedDataset.scala:79)\\n\\t\\tat org.apache.spark.sql.RelationalGroupedDataset.agg(RelationalGroupedDataset.scala:253)\\n\\t\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\\t\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\\n\\t\\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\t\\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\\n\\t\\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\\n\\t\\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\\n\\t\\tat py4j.Gateway.invoke(Gateway.java:306)\\n\\t\\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\\n\\t\\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\\n\\t\\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:199)\\n\\t\\tat py4j.ClientServerConnection.run(ClientServerConnection.java:119)\\n\\t\\tat java.base/java.lang.Thread.run(Thread.java:840)\\n\", \"stacktrace\": [\"Traceback (most recent call last):\", \"  File \\\"/databricks/spark/python/pyspark/errors/exceptions/captured.py\\\", line 263, in deco\", \"    return f(*a, **kw)\", \"           ^^^^^^^^^^^\", \"  File \\\"/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py\\\", line 326, in get_return_value\", \"    raise Py4JJavaError(\", \"py4j.protocol.Py4JJavaError: An error occurred while calling o22035.agg.\", \": org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `Airline` cannot be resolved. Did you mean one of the following? [`AirTime`, `ArrTime`, `Origin`, `Airline_enc`, `TaxiIn`]. SQLSTATE: 42703;\", \"'Aggregate ['Airline], ['Airline, count(1) AS TotalFlights#21073L, count(CASE WHEN (Delayed#2400 = 1) THEN true END) AS DelayedFlights#21074L]\", \"+- Project [FlightDate#2360, Origin#2362, Dest#2363, Cancelled#2364, Diverted#2365, CRSDepTime#2366, DepTime#2367, DepDelayMinutes#2368, DepDelay#2369, ArrTime#2370, ArrDelayMinutes#2371, AirTime#2372, CRSElapsedTime#2373, ActualElapsedTime#2374, Distance#2375, Year#2376, Quarter#2377, Month#2378, DayofMonth#2379, DayOfWeek#2380, Operating_Airline#2381, OriginAirportID#2382, OriginCityName#2383, OriginStateName#2384, ... 17 more fields]\", \"   +- Project [Airline#2361, FlightDate#2360, Origin#2362, Dest#2363, Cancelled#2364, Diverted#2365, CRSDepTime#2366, DepTime#2367, DepDelayMinutes#2368, DepDelay#2369, ArrTime#2370, ArrDelayMinutes#2371, AirTime#2372, CRSElapsedTime#2373, ActualElapsedTime#2374, Distance#2375, Year#2376, Quarter#2377, Month#2378, DayofMonth#2379, DayOfWeek#2380, Operating_Airline#2381, OriginAirportID#2382, OriginCityName#2383, ... 18 more fields]\", \"      +- Join LeftOuter, (Airline#2361 = Airline#18448)\", \"         :- Relation [FlightDate#2360,Airline#2361,Origin#2362,Dest#2363,Cancelled#2364,Diverted#2365,CRSDepTime#2366,DepTime#2367,DepDelayMinutes#2368,DepDelay#2369,ArrTime#2370,ArrDelayMinutes#2371,AirTime#2372,CRSElapsedTime#2373,ActualElapsedTime#2374,Distance#2375,Year#2376,Quarter#2377,Month#2378,DayofMonth#2379,DayOfWeek#2380,Operating_Airline#2381,OriginAirportID#2382,OriginCityName#2383,... 17 more fields] csv\", \"         +- Aggregate [Airline#18448], [Airline#18448, avg(Delayed#18487) AS Airline_enc#18443]\", \"            +- Relation [FlightDate#18447,Airline#18448,Origin#18449,Dest#18450,Cancelled#18451,Diverted#18452,CRSDepTime#18453,DepTime#18454,DepDelayMinutes#18455,DepDelay#18456,ArrTime#18457,ArrDelayMinutes#18458,AirTime#18459,CRSElapsedTime#18460,ActualElapsedTime#18461,Distance#18462,Year#18463,Quarter#18464,Month#18465,DayofMonth#18466,DayOfWeek#18467,Operating_Airline#18468,OriginAirportID#18469,OriginCityName#18470,... 17 more fields] csv\", \"\", \"\\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:481)\", \"\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:181)\", \"\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$8(CheckAnalysis.scala:421)\", \"\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$8$adapted(CheckAnalysis.scala:406)\", \"\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:287)\", \"\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7(CheckAnalysis.scala:406)\", \"\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7$adapted(CheckAnalysis.scala:406)\", \"\\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\", \"\\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\", \"\\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\", \"\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:406)\", \"\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:286)\", \"\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:287)\", \"\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:286)\", \"\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:261)\", \"\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:406)\", \"\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:246)\", \"\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\", \"\\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\", \"\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:233)\", \"\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:233)\", \"\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:406)\", \"\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:171)\", \"\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\", \"\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:201)\", \"\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:171)\", \"\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:68)\", \"\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:94)\", \"\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:63)\", \"\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:473)\", \"\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:443)\", \"\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:473)\", \"\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:276)\", \"\\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\", \"\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:483)\", \"\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:593)\", \"\\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:145)\", \"\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:593)\", \"\\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1224)\", \"\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:592)\", \"\\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\", \"\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:588)\", \"\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1384)\", \"\\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:588)\", \"\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:270)\", \"\\tat scala.util.Try$.apply(Try.scala:213)\", \"\\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1676)\", \"\\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1737)\", \"\\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\", \"\\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:297)\", \"\\tat org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfos(DatasourceAttributeExtractor.scala:88)\", \"\\tat org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfos$(DatasourceAttributeExtractor.scala:85)\", \"\\tat org.mlflow.spark.autologging.ReplAwareDatasourceAttributeExtractor$.getTableInfos(DatasourceAttributeExtractor.scala:142)\", \"\\tat org.mlflow.spark.autologging.ReplAwareSparkDataSourceListener.onSQLExecutionEnd(ReplAwareSparkDataSourceListener.scala:49)\", \"\\tat org.mlflow.spark.autologging.SparkDataSourceListener.$anonfun$onOtherEvent$1(SparkDataSourceListener.scala:39)\", \"\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\", \"\\tat org.mlflow.spark.autologging.ExceptionUtils$.tryAndLogUnexpectedError(ExceptionUtils.scala:26)\", \"\\tat org.mlflow.spark.autologging.SparkDataSourceListener.onOtherEvent(SparkDataSourceListener.scala:39)\", \"\\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:108)\", \"\\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\", \"\\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:42)\", \"\\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:42)\", \"\\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:199)\", \"\\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:169)\", \"\\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:116)\", \"\\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:116)\", \"\\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\", \"\\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\", \"\\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:111)\", \"\\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:107)\", \"\\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1564)\", \"\\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:107)\", \"\\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\", \"\\t\\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:481)\", \"\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:181)\", \"\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$8(CheckAnalysis.scala:421)\", \"\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$8$adapted(CheckAnalysis.scala:406)\", \"\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:287)\", \"\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7(CheckAnalysis.scala:406)\", \"\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7$adapted(CheckAnalysis.scala:406)\", \"\\t\\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\", \"\\t\\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\", \"\\t\\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\", \"\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:406)\", \"\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:286)\", \"\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:287)\", \"\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:286)\", \"\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:261)\", \"\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:406)\", \"\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:246)\", \"\\t\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\", \"\\t\\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\", \"\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:233)\", \"\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:233)\", \"\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:406)\", \"\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:171)\", \"\\t\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\", \"\\t\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:201)\", \"\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:171)\", \"\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:68)\", \"\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:94)\", \"\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:63)\", \"\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:473)\", \"\\t\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:443)\", \"\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:473)\", \"\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:276)\", \"\\t\\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\", \"\\t\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:483)\", \"\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:593)\", \"\\t\\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:145)\", \"\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:593)\", \"\\t\\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1224)\", \"\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:592)\", \"\\t\\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\", \"\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:588)\", \"\\t\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1384)\", \"\\t\\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:588)\", \"\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:270)\", \"\\t\\tat scala.util.Try$.apply(Try.scala:213)\", \"\\t\\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1676)\", \"\\t\\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\", \"\\t\\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\", \"\\t\\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\", \"\\t\\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:297)\", \"\\t\\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:251)\", \"\\t\\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:106)\", \"\\t\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1384)\", \"\\t\\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1391)\", \"\\t\\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\", \"\\t\\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1391)\", \"\\t\\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:104)\", \"\\t\\tat org.apache.spark.sql.RelationalGroupedDataset.toDF(RelationalGroupedDataset.scala:79)\", \"\\t\\tat org.apache.spark.sql.RelationalGroupedDataset.agg(RelationalGroupedDataset.scala:253)\", \"\\t\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\", \"\\t\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\", \"\\t\\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\", \"\\t\\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\", \"\\t\\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\", \"\\t\\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\", \"\\t\\tat py4j.Gateway.invoke(Gateway.java:306)\", \"\\t\\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\", \"\\t\\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\", \"\\t\\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:199)\", \"\\t\\tat py4j.ClientServerConnection.run(ClientServerConnection.java:119)\", \"\\t\\tat java.base/java.lang.Thread.run(Thread.java:840)\"]}}\nERROR:DataFrameQueryContextLogger:[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `Airline` cannot be resolved. Did you mean one of the following? [`AirTime`, `ArrTime`, `Origin`, `Airline_enc`, `TaxiIn`]. SQLSTATE: 42703\nTraceback (most recent call last):\n  File \"/databricks/spark/python/pyspark/errors/exceptions/captured.py\", line 263, in deco\n    return f(*a, **kw)\n           ^^^^^^^^^^^\n  File \"/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py\", line 326, in get_return_value\n    raise Py4JJavaError(\npy4j.protocol.Py4JJavaError: An error occurred while calling o22035.agg.\n: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `Airline` cannot be resolved. Did you mean one of the following? [`AirTime`, `ArrTime`, `Origin`, `Airline_enc`, `TaxiIn`]. SQLSTATE: 42703;\n'Aggregate ['Airline], ['Airline, count(1) AS TotalFlights#21073L, count(CASE WHEN (Delayed#2400 = 1) THEN true END) AS DelayedFlights#21074L]\n+- Project [FlightDate#2360, Origin#2362, Dest#2363, Cancelled#2364, Diverted#2365, CRSDepTime#2366, DepTime#2367, DepDelayMinutes#2368, DepDelay#2369, ArrTime#2370, ArrDelayMinutes#2371, AirTime#2372, CRSElapsedTime#2373, ActualElapsedTime#2374, Distance#2375, Year#2376, Quarter#2377, Month#2378, DayofMonth#2379, DayOfWeek#2380, Operating_Airline#2381, OriginAirportID#2382, OriginCityName#2383, OriginStateName#2384, ... 17 more fields]\n   +- Project [Airline#2361, FlightDate#2360, Origin#2362, Dest#2363, Cancelled#2364, Diverted#2365, CRSDepTime#2366, DepTime#2367, DepDelayMinutes#2368, DepDelay#2369, ArrTime#2370, ArrDelayMinutes#2371, AirTime#2372, CRSElapsedTime#2373, ActualElapsedTime#2374, Distance#2375, Year#2376, Quarter#2377, Month#2378, DayofMonth#2379, DayOfWeek#2380, Operating_Airline#2381, OriginAirportID#2382, OriginCityName#2383, ... 18 more fields]\n      +- Join LeftOuter, (Airline#2361 = Airline#18448)\n         :- Relation [FlightDate#2360,Airline#2361,Origin#2362,Dest#2363,Cancelled#2364,Diverted#2365,CRSDepTime#2366,DepTime#2367,DepDelayMinutes#2368,DepDelay#2369,ArrTime#2370,ArrDelayMinutes#2371,AirTime#2372,CRSElapsedTime#2373,ActualElapsedTime#2374,Distance#2375,Year#2376,Quarter#2377,Month#2378,DayofMonth#2379,DayOfWeek#2380,Operating_Airline#2381,OriginAirportID#2382,OriginCityName#2383,... 17 more fields] csv\n         +- Aggregate [Airline#18448], [Airline#18448, avg(Delayed#18487) AS Airline_enc#18443]\n            +- Relation [FlightDate#18447,Airline#18448,Origin#18449,Dest#18450,Cancelled#18451,Diverted#18452,CRSDepTime#18453,DepTime#18454,DepDelayMinutes#18455,DepDelay#18456,ArrTime#18457,ArrDelayMinutes#18458,AirTime#18459,CRSElapsedTime#18460,ActualElapsedTime#18461,Distance#18462,Year#18463,Quarter#18464,Month#18465,DayofMonth#18466,DayOfWeek#18467,Operating_Airline#18468,OriginAirportID#18469,OriginCityName#18470,... 17 more fields] csv\n\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:481)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:181)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$8(CheckAnalysis.scala:421)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$8$adapted(CheckAnalysis.scala:406)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:287)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7(CheckAnalysis.scala:406)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7$adapted(CheckAnalysis.scala:406)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:406)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:286)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:287)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:286)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:261)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:406)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:246)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:233)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:233)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:406)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:171)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:171)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:68)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:94)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:63)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:473)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:443)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:473)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:276)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:483)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:593)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:145)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:593)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1224)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:592)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:588)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1384)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:588)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:270)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1676)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1737)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:297)\n\tat org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfos(DatasourceAttributeExtractor.scala:88)\n\tat org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfos$(DatasourceAttributeExtractor.scala:85)\n\tat org.mlflow.spark.autologging.ReplAwareDatasourceAttributeExtractor$.getTableInfos(DatasourceAttributeExtractor.scala:142)\n\tat org.mlflow.spark.autologging.ReplAwareSparkDataSourceListener.onSQLExecutionEnd(ReplAwareSparkDataSourceListener.scala:49)\n\tat org.mlflow.spark.autologging.SparkDataSourceListener.$anonfun$onOtherEvent$1(SparkDataSourceListener.scala:39)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.mlflow.spark.autologging.ExceptionUtils$.tryAndLogUnexpectedError(ExceptionUtils.scala:26)\n\tat org.mlflow.spark.autologging.SparkDataSourceListener.onOtherEvent(SparkDataSourceListener.scala:39)\n\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:108)\n\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:42)\n\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:42)\n\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:199)\n\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:169)\n\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:116)\n\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:116)\n\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:111)\n\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:107)\n\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1564)\n\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:107)\n\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\n\t\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:481)\n\t\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:181)\n\t\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$8(CheckAnalysis.scala:421)\n\t\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$8$adapted(CheckAnalysis.scala:406)\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:287)\n\t\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7(CheckAnalysis.scala:406)\n\t\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7$adapted(CheckAnalysis.scala:406)\n\t\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\t\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\t\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\t\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:406)\n\t\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:286)\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:287)\n\t\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:286)\n\t\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:261)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:406)\n\t\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:246)\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:233)\n\t\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:233)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:406)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:171)\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:201)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:171)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:68)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:94)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:63)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:473)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:443)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:473)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:276)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:483)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:593)\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:145)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:593)\n\t\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1224)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:592)\n\t\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:588)\n\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1384)\n\t\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:588)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:270)\n\t\tat scala.util.Try$.apply(Try.scala:213)\n\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1676)\n\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n\t\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\t\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:297)\n\t\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:251)\n\t\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:106)\n\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1384)\n\t\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1391)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1391)\n\t\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:104)\n\t\tat org.apache.spark.sql.RelationalGroupedDataset.toDF(RelationalGroupedDataset.scala:79)\n\t\tat org.apache.spark.sql.RelationalGroupedDataset.agg(RelationalGroupedDataset.scala:253)\n\t\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\t\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\t\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\t\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\t\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\t\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\t\tat py4j.Gateway.invoke(Gateway.java:306)\n\t\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\t\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\t\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:199)\n\t\tat py4j.ClientServerConnection.run(ClientServerConnection.java:119)\n\t\tat java.base/java.lang.Thread.run(Thread.java:840)\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-5658781754548476>, line 4\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfunctions\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m col, count, when\n",
       "\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# Calculate total flights and delayed flights per airline\u001B[39;00m\n",
       "\u001B[0;32m----> 4\u001B[0m df_delays \u001B[38;5;241m=\u001B[39m df\u001B[38;5;241m.\u001B[39mgroupBy(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAirline\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39magg(\n",
       "\u001B[1;32m      5\u001B[0m     count(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m*\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTotalFlights\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n",
       "\u001B[1;32m      6\u001B[0m     count(when(col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDelayed\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m, \u001B[38;5;28;01mTrue\u001B[39;00m))\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDelayedFlights\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m      7\u001B[0m )\n",
       "\u001B[1;32m      9\u001B[0m \u001B[38;5;66;03m# Calculate delay percentage\u001B[39;00m\n",
       "\u001B[1;32m     10\u001B[0m df_delays \u001B[38;5;241m=\u001B[39m df_delays\u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDelayPercentage\u001B[39m\u001B[38;5;124m\"\u001B[39m, (col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDelayedFlights\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m/\u001B[39m col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTotalFlights\u001B[39m\u001B[38;5;124m\"\u001B[39m)) \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m100\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:47\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     45\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 47\u001B[0m     res \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m     48\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     49\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     50\u001B[0m     )\n",
       "\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/group.py:188\u001B[0m, in \u001B[0;36mGroupedData.agg\u001B[0;34m(self, *exprs)\u001B[0m\n",
       "\u001B[1;32m    186\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mall\u001B[39m(\u001B[38;5;28misinstance\u001B[39m(c, Column) \u001B[38;5;28;01mfor\u001B[39;00m c \u001B[38;5;129;01min\u001B[39;00m exprs), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mall exprs should be Column\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    187\u001B[0m     exprs \u001B[38;5;241m=\u001B[39m cast(Tuple[Column, \u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m], exprs)\n",
       "\u001B[0;32m--> 188\u001B[0m     jdf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jgd\u001B[38;5;241m.\u001B[39magg(exprs[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39m_jc, _to_seq(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msession\u001B[38;5;241m.\u001B[39m_sc, [c\u001B[38;5;241m.\u001B[39m_jc \u001B[38;5;28;01mfor\u001B[39;00m c \u001B[38;5;129;01min\u001B[39;00m exprs[\u001B[38;5;241m1\u001B[39m:]]))\n",
       "\u001B[1;32m    189\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(jdf, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msession)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1355\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1349\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1350\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1351\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1352\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1354\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1355\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n",
       "\u001B[1;32m   1356\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n",
       "\u001B[1;32m   1358\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1359\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:269\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    265\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    266\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    267\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    268\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 269\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m    270\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    271\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `Airline` cannot be resolved. Did you mean one of the following? [`AirTime`, `ArrTime`, `Origin`, `Airline_enc`, `TaxiIn`]. SQLSTATE: 42703;\n",
       "'Aggregate ['Airline], ['Airline, count(1) AS TotalFlights#21073L, count(CASE WHEN (Delayed#2400 = 1) THEN true END) AS DelayedFlights#21074L]\n",
       "+- Project [FlightDate#2360, Origin#2362, Dest#2363, Cancelled#2364, Diverted#2365, CRSDepTime#2366, DepTime#2367, DepDelayMinutes#2368, DepDelay#2369, ArrTime#2370, ArrDelayMinutes#2371, AirTime#2372, CRSElapsedTime#2373, ActualElapsedTime#2374, Distance#2375, Year#2376, Quarter#2377, Month#2378, DayofMonth#2379, DayOfWeek#2380, Operating_Airline#2381, OriginAirportID#2382, OriginCityName#2383, OriginStateName#2384, ... 17 more fields]\n",
       "   +- Project [Airline#2361, FlightDate#2360, Origin#2362, Dest#2363, Cancelled#2364, Diverted#2365, CRSDepTime#2366, DepTime#2367, DepDelayMinutes#2368, DepDelay#2369, ArrTime#2370, ArrDelayMinutes#2371, AirTime#2372, CRSElapsedTime#2373, ActualElapsedTime#2374, Distance#2375, Year#2376, Quarter#2377, Month#2378, DayofMonth#2379, DayOfWeek#2380, Operating_Airline#2381, OriginAirportID#2382, OriginCityName#2383, ... 18 more fields]\n",
       "      +- Join LeftOuter, (Airline#2361 = Airline#18448)\n",
       "         :- Relation [FlightDate#2360,Airline#2361,Origin#2362,Dest#2363,Cancelled#2364,Diverted#2365,CRSDepTime#2366,DepTime#2367,DepDelayMinutes#2368,DepDelay#2369,ArrTime#2370,ArrDelayMinutes#2371,AirTime#2372,CRSElapsedTime#2373,ActualElapsedTime#2374,Distance#2375,Year#2376,Quarter#2377,Month#2378,DayofMonth#2379,DayOfWeek#2380,Operating_Airline#2381,OriginAirportID#2382,OriginCityName#2383,... 17 more fields] csv\n",
       "         +- Aggregate [Airline#18448], [Airline#18448, avg(Delayed#18487) AS Airline_enc#18443]\n",
       "            +- Relation [FlightDate#18447,Airline#18448,Origin#18449,Dest#18450,Cancelled#18451,Diverted#18452,CRSDepTime#18453,DepTime#18454,DepDelayMinutes#18455,DepDelay#18456,ArrTime#18457,ArrDelayMinutes#18458,AirTime#18459,CRSElapsedTime#18460,ActualElapsedTime#18461,Distance#18462,Year#18463,Quarter#18464,Month#18465,DayofMonth#18466,DayOfWeek#18467,Operating_Airline#18468,OriginAirportID#18469,OriginCityName#18470,... 17 more fields] csv\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "AnalysisException",
        "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `Airline` cannot be resolved. Did you mean one of the following? [`AirTime`, `ArrTime`, `Origin`, `Airline_enc`, `TaxiIn`]. SQLSTATE: 42703;\n'Aggregate ['Airline], ['Airline, count(1) AS TotalFlights#21073L, count(CASE WHEN (Delayed#2400 = 1) THEN true END) AS DelayedFlights#21074L]\n+- Project [FlightDate#2360, Origin#2362, Dest#2363, Cancelled#2364, Diverted#2365, CRSDepTime#2366, DepTime#2367, DepDelayMinutes#2368, DepDelay#2369, ArrTime#2370, ArrDelayMinutes#2371, AirTime#2372, CRSElapsedTime#2373, ActualElapsedTime#2374, Distance#2375, Year#2376, Quarter#2377, Month#2378, DayofMonth#2379, DayOfWeek#2380, Operating_Airline#2381, OriginAirportID#2382, OriginCityName#2383, OriginStateName#2384, ... 17 more fields]\n   +- Project [Airline#2361, FlightDate#2360, Origin#2362, Dest#2363, Cancelled#2364, Diverted#2365, CRSDepTime#2366, DepTime#2367, DepDelayMinutes#2368, DepDelay#2369, ArrTime#2370, ArrDelayMinutes#2371, AirTime#2372, CRSElapsedTime#2373, ActualElapsedTime#2374, Distance#2375, Year#2376, Quarter#2377, Month#2378, DayofMonth#2379, DayOfWeek#2380, Operating_Airline#2381, OriginAirportID#2382, OriginCityName#2383, ... 18 more fields]\n      +- Join LeftOuter, (Airline#2361 = Airline#18448)\n         :- Relation [FlightDate#2360,Airline#2361,Origin#2362,Dest#2363,Cancelled#2364,Diverted#2365,CRSDepTime#2366,DepTime#2367,DepDelayMinutes#2368,DepDelay#2369,ArrTime#2370,ArrDelayMinutes#2371,AirTime#2372,CRSElapsedTime#2373,ActualElapsedTime#2374,Distance#2375,Year#2376,Quarter#2377,Month#2378,DayofMonth#2379,DayOfWeek#2380,Operating_Airline#2381,OriginAirportID#2382,OriginCityName#2383,... 17 more fields] csv\n         +- Aggregate [Airline#18448], [Airline#18448, avg(Delayed#18487) AS Airline_enc#18443]\n            +- Relation [FlightDate#18447,Airline#18448,Origin#18449,Dest#18450,Cancelled#18451,Diverted#18452,CRSDepTime#18453,DepTime#18454,DepDelayMinutes#18455,DepDelay#18456,ArrTime#18457,ArrDelayMinutes#18458,AirTime#18459,CRSElapsedTime#18460,ActualElapsedTime#18461,Distance#18462,Year#18463,Quarter#18464,Month#18465,DayofMonth#18466,DayOfWeek#18467,Operating_Airline#18468,OriginAirportID#18469,OriginCityName#18470,... 17 more fields] csv\n"
       },
       "metadata": {
        "errorSummary": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `Airline` cannot be resolved. Did you mean one of the following? [`AirTime`, `ArrTime`, `Origin`, `Airline_enc`, `TaxiIn`]. SQLSTATE: 42703\n== DataFrame ==\n\"col\" was called from jdk.internal.reflect.GeneratedMethodAccessor344.invoke(Unknown Source)\n"
       },
       "removedWidgets": [],
       "sqlProps": {
        "errorClass": "UNRESOLVED_COLUMN.WITH_SUGGESTION",
        "pysparkCallSite": "jdk.internal.reflect.GeneratedMethodAccessor344.invoke(Unknown Source)",
        "pysparkFragment": "col",
        "sqlState": "42703",
        "stackTrace": null,
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)",
        "File \u001B[0;32m<command-5658781754548476>, line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfunctions\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m col, count, when\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# Calculate total flights and delayed flights per airline\u001B[39;00m\n\u001B[0;32m----> 4\u001B[0m df_delays \u001B[38;5;241m=\u001B[39m df\u001B[38;5;241m.\u001B[39mgroupBy(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAirline\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39magg(\n\u001B[1;32m      5\u001B[0m     count(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m*\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTotalFlights\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m      6\u001B[0m     count(when(col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDelayed\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m, \u001B[38;5;28;01mTrue\u001B[39;00m))\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDelayedFlights\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      7\u001B[0m )\n\u001B[1;32m      9\u001B[0m \u001B[38;5;66;03m# Calculate delay percentage\u001B[39;00m\n\u001B[1;32m     10\u001B[0m df_delays \u001B[38;5;241m=\u001B[39m df_delays\u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDelayPercentage\u001B[39m\u001B[38;5;124m\"\u001B[39m, (col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDelayedFlights\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m/\u001B[39m col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTotalFlights\u001B[39m\u001B[38;5;124m\"\u001B[39m)) \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m100\u001B[39m)\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:47\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     45\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 47\u001B[0m     res \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m     48\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     49\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     50\u001B[0m     )\n\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/group.py:188\u001B[0m, in \u001B[0;36mGroupedData.agg\u001B[0;34m(self, *exprs)\u001B[0m\n\u001B[1;32m    186\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mall\u001B[39m(\u001B[38;5;28misinstance\u001B[39m(c, Column) \u001B[38;5;28;01mfor\u001B[39;00m c \u001B[38;5;129;01min\u001B[39;00m exprs), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mall exprs should be Column\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    187\u001B[0m     exprs \u001B[38;5;241m=\u001B[39m cast(Tuple[Column, \u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m], exprs)\n\u001B[0;32m--> 188\u001B[0m     jdf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jgd\u001B[38;5;241m.\u001B[39magg(exprs[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39m_jc, _to_seq(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msession\u001B[38;5;241m.\u001B[39m_sc, [c\u001B[38;5;241m.\u001B[39m_jc \u001B[38;5;28;01mfor\u001B[39;00m c \u001B[38;5;129;01min\u001B[39;00m exprs[\u001B[38;5;241m1\u001B[39m:]]))\n\u001B[1;32m    189\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(jdf, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msession)\n",
        "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1355\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1349\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1350\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1351\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1352\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1354\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1355\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n\u001B[1;32m   1356\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n\u001B[1;32m   1358\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1359\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:269\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    265\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    266\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    267\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    268\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 269\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    270\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    271\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
        "\u001B[0;31mAnalysisException\u001B[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `Airline` cannot be resolved. Did you mean one of the following? [`AirTime`, `ArrTime`, `Origin`, `Airline_enc`, `TaxiIn`]. SQLSTATE: 42703;\n'Aggregate ['Airline], ['Airline, count(1) AS TotalFlights#21073L, count(CASE WHEN (Delayed#2400 = 1) THEN true END) AS DelayedFlights#21074L]\n+- Project [FlightDate#2360, Origin#2362, Dest#2363, Cancelled#2364, Diverted#2365, CRSDepTime#2366, DepTime#2367, DepDelayMinutes#2368, DepDelay#2369, ArrTime#2370, ArrDelayMinutes#2371, AirTime#2372, CRSElapsedTime#2373, ActualElapsedTime#2374, Distance#2375, Year#2376, Quarter#2377, Month#2378, DayofMonth#2379, DayOfWeek#2380, Operating_Airline#2381, OriginAirportID#2382, OriginCityName#2383, OriginStateName#2384, ... 17 more fields]\n   +- Project [Airline#2361, FlightDate#2360, Origin#2362, Dest#2363, Cancelled#2364, Diverted#2365, CRSDepTime#2366, DepTime#2367, DepDelayMinutes#2368, DepDelay#2369, ArrTime#2370, ArrDelayMinutes#2371, AirTime#2372, CRSElapsedTime#2373, ActualElapsedTime#2374, Distance#2375, Year#2376, Quarter#2377, Month#2378, DayofMonth#2379, DayOfWeek#2380, Operating_Airline#2381, OriginAirportID#2382, OriginCityName#2383, ... 18 more fields]\n      +- Join LeftOuter, (Airline#2361 = Airline#18448)\n         :- Relation [FlightDate#2360,Airline#2361,Origin#2362,Dest#2363,Cancelled#2364,Diverted#2365,CRSDepTime#2366,DepTime#2367,DepDelayMinutes#2368,DepDelay#2369,ArrTime#2370,ArrDelayMinutes#2371,AirTime#2372,CRSElapsedTime#2373,ActualElapsedTime#2374,Distance#2375,Year#2376,Quarter#2377,Month#2378,DayofMonth#2379,DayOfWeek#2380,Operating_Airline#2381,OriginAirportID#2382,OriginCityName#2383,... 17 more fields] csv\n         +- Aggregate [Airline#18448], [Airline#18448, avg(Delayed#18487) AS Airline_enc#18443]\n            +- Relation [FlightDate#18447,Airline#18448,Origin#18449,Dest#18450,Cancelled#18451,Diverted#18452,CRSDepTime#18453,DepTime#18454,DepDelayMinutes#18455,DepDelay#18456,ArrTime#18457,ArrDelayMinutes#18458,AirTime#18459,CRSElapsedTime#18460,ActualElapsedTime#18461,Distance#18462,Year#18463,Quarter#18464,Month#18465,DayofMonth#18466,DayOfWeek#18467,Operating_Airline#18468,OriginAirportID#18469,OriginCityName#18470,... 17 more fields] csv\n"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, count, when\n",
    "\n",
    "# Calculate total flights and delayed flights per airline\n",
    "df_delays = df.groupBy(\"Airline\").agg(\n",
    "    count(\"*\").alias(\"TotalFlights\"),\n",
    "    count(when(col(\"Delayed\") == 1, True)).alias(\"DelayedFlights\")\n",
    ")\n",
    "\n",
    "# Calculate delay percentage\n",
    "df_delays = df_delays.withColumn(\"DelayPercentage\", (col(\"DelayedFlights\") / col(\"TotalFlights\")) * 100)\n",
    "\n",
    "# Sort in descending order of delay percentage\n",
    "df_delays.orderBy(col(\"DelayPercentage\").desc()).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eceb4a83-56bd-450a-87c0-ae4ebf26ca1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_delays.orderBy(col(\"DelayPercentage\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d20341a0-017d-42e2-a305-65aca2352507",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+--------------+-----------------+\n|Origin|TotalFlights|DelayedFlights|  DelayPercentage|\n+------+------------+--------------+-----------------+\n|   HYA|           1|             1|            100.0|\n|   PQI|           2|             2|            100.0|\n|   CDB|           1|             1|            100.0|\n|   LCH|          11|             7|63.63636363636363|\n|   RFD|           8|             4|             50.0|\n|   DUT|           2|             1|             50.0|\n|   MKK|           4|             2|             50.0|\n|   SCK|           6|             3|             50.0|\n|   SPI|           8|             4|             50.0|\n|   MVY|           2|             1|             50.0|\n|   GCK|           2|             1|             50.0|\n|   BFF|           2|             1|             50.0|\n|   USA|          13|             6|46.15384615384615|\n|   OTH|           5|             2|             40.0|\n|   BGM|           5|             2|             40.0|\n|   STX|          10|             4|             40.0|\n|   FNT|          40|            15|             37.5|\n|   SWF|          11|             4|36.36363636363637|\n|   FSM|          11|             4|36.36363636363637|\n|   EKO|           6|             2|33.33333333333333|\n+------+------------+--------------+-----------------+\nonly showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "## To check for the Origin \n",
    "\n",
    "from pyspark.sql.functions import col, count, when\n",
    "\n",
    "# Calculate total flights and delayed flights per origin airport\n",
    "df_origin_delay = df.groupBy(\"Origin\").agg(\n",
    "    count(\"*\").alias(\"TotalFlights\"),\n",
    "    count(when(col(\"Delayed\") == 1, True)).alias(\"DelayedFlights\")\n",
    ")\n",
    "\n",
    "# Calculate delay percentage\n",
    "df_origin_delay = df_origin_delay.withColumn(\"DelayPercentage\", (col(\"DelayedFlights\") / col(\"TotalFlights\")) * 100)\n",
    "\n",
    "# Sort in descending order of delay percentage\n",
    "df_origin_delay.orderBy(col(\"DelayPercentage\").desc()).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af2bd848-f67a-4edd-a452-358c77027d50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n\n--- Results for column: Origin ---\n+------+------------+--------------+-----------------+\n|Origin|TotalFlights|DelayedFlights|DelayPercentage  |\n+------+------------+--------------+-----------------+\n|HYA   |1           |1             |100.0            |\n|PQI   |2           |2             |100.0            |\n|CDB   |1           |1             |100.0            |\n|LCH   |11          |7             |63.63636363636363|\n|RFD   |8           |4             |50.0             |\n|DUT   |2           |1             |50.0             |\n|MKK   |4           |2             |50.0             |\n|SCK   |6           |3             |50.0             |\n|SPI   |8           |4             |50.0             |\n|MVY   |2           |1             |50.0             |\n|GCK   |2           |1             |50.0             |\n|BFF   |2           |1             |50.0             |\n|USA   |13          |6             |46.15384615384615|\n|OTH   |5           |2             |40.0             |\n|BGM   |5           |2             |40.0             |\n|STX   |10          |4             |40.0             |\n|FNT   |40          |15            |37.5             |\n|SWF   |11          |4             |36.36363636363637|\n|FSM   |11          |4             |36.36363636363637|\n|EKO   |6           |2             |33.33333333333333|\n+------+------------+--------------+-----------------+\nonly showing top 20 rows\n\n\n--- Results for column: Dest ---\n+----+------------+--------------+------------------+\n|Dest|TotalFlights|DelayedFlights|DelayPercentage   |\n+----+------------+--------------+------------------+\n|DUT |2           |2             |100.0             |\n|CYS |1           |1             |100.0             |\n|ADK |1           |1             |100.0             |\n|EKO |1           |1             |100.0             |\n|DIK |4           |3             |75.0              |\n|DBQ |3           |2             |66.66666666666666 |\n|SMX |3           |2             |66.66666666666666 |\n|PSE |2           |1             |50.0              |\n|ACK |4           |2             |50.0              |\n|ESC |4           |2             |50.0              |\n|PIB |4           |2             |50.0              |\n|DDC |4           |2             |50.0              |\n|BJI |4           |2             |50.0              |\n|MKG |2           |1             |50.0              |\n|BLV |2           |1             |50.0              |\n|WYS |2           |1             |50.0              |\n|PGD |48          |22            |45.83333333333333 |\n|RHI |9           |4             |44.44444444444444 |\n|ABY |7           |3             |42.857142857142854|\n|GFK |12          |5             |41.66666666666667 |\n+----+------------+--------------+------------------+\nonly showing top 20 rows\n\n\n--- Results for column: Cancelled ---\n+---------+------------+--------------+-----------------+\n|Cancelled|TotalFlights|DelayedFlights|DelayPercentage  |\n+---------+------------+--------------+-----------------+\n|false    |48417       |8396          |17.34101658508375|\n+---------+------------+--------------+-----------------+\n\n\n\n--- Results for column: Diverted ---\n+--------+------------+--------------+-----------------+\n|Diverted|TotalFlights|DelayedFlights|DelayPercentage  |\n+--------+------------+--------------+-----------------+\n|false   |48417       |8396          |17.34101658508375|\n+--------+------------+--------------+-----------------+\n\n\n\n--- Results for column: Distance ---\n+--------+------------+--------------+---------------+\n|Distance|TotalFlights|DelayedFlights|DelayPercentage|\n+--------+------------+--------------+---------------+\n|2297.0  |1           |1             |100.0          |\n|2511.0  |1           |1             |100.0          |\n|1649.0  |1           |1             |100.0          |\n|914.0   |1           |1             |100.0          |\n|498.0   |1           |1             |100.0          |\n|2154.0  |2           |2             |100.0          |\n|915.0   |2           |2             |100.0          |\n|1004.0  |2           |2             |100.0          |\n|2979.0  |1           |1             |100.0          |\n|1224.0  |1           |1             |100.0          |\n|1279.0  |1           |1             |100.0          |\n|2078.0  |1           |1             |100.0          |\n|1639.0  |1           |1             |100.0          |\n|1435.0  |1           |1             |100.0          |\n|1245.0  |1           |1             |100.0          |\n|1993.0  |2           |2             |100.0          |\n|4757.0  |1           |1             |100.0          |\n|1198.0  |1           |1             |100.0          |\n|80.0    |1           |1             |100.0          |\n|1394.0  |1           |1             |100.0          |\n+--------+------------+--------------+---------------+\nonly showing top 20 rows\n\n\n--- Results for column: Year ---\n+----+------------+--------------+------------------+\n|Year|TotalFlights|DelayedFlights|DelayPercentage   |\n+----+------------+--------------+------------------+\n|2022|9646        |2032          |21.065726726104085|\n|2018|9815        |1902          |19.37850229240958 |\n|2019|9769        |1865          |19.09100214965708 |\n|2021|9811        |1646          |16.77708694322699 |\n|2020|9376        |951           |10.142918088737202|\n+----+------------+--------------+------------------+\n\n\n\n--- Results for column: Quarter ---\n+-------+------------+--------------+------------------+\n|Quarter|TotalFlights|DelayedFlights|DelayPercentage   |\n+-------+------------+--------------+------------------+\n|2      |12326       |2256          |18.30277462274866 |\n|3      |11661       |2077          |17.81150844695995 |\n|1      |13584       |2304          |16.96113074204947 |\n|4      |10846       |1759          |16.217960538447354|\n+-------+------------+--------------+------------------+\n\n\n\n--- Results for column: Month ---\n+-----+------------+--------------+------------------+\n|Month|TotalFlights|DelayedFlights|DelayPercentage   |\n+-----+------------+--------------+------------------+\n|6    |4162        |904           |21.720326765977894|\n|7    |4650        |911           |19.591397849462368|\n|8    |3447        |668           |19.379170293008414|\n|2    |4031        |758           |18.80426693128256 |\n|12   |3655        |654           |17.89329685362517 |\n|5    |4127        |725           |17.567240125999515|\n|1    |4730        |802           |16.955602536997887|\n|10   |3548        |571           |16.09357384441939 |\n|4    |4037        |627           |15.531335149863759|\n|3    |4823        |744           |15.426083350611652|\n|11   |3643        |534           |14.658248696129563|\n|9    |3564        |498           |13.973063973063974|\n+-----+------------+--------------+------------------+\n\n\n\n--- Results for column: DayofMonth ---\n+----------+------------+--------------+------------------+\n|DayofMonth|TotalFlights|DelayedFlights|DelayPercentage   |\n+----------+------------+--------------+------------------+\n|13        |1633        |317           |19.412124923453767|\n|1         |1553        |301           |19.381841596909208|\n|16        |1583        |306           |19.33038534428301 |\n|15        |1561        |300           |19.218449711723252|\n|2         |1575        |298           |18.92063492063492 |\n|19        |1680        |308           |18.333333333333332|\n|11        |1645        |299           |18.17629179331307 |\n|14        |1587        |285           |17.958412098298677|\n|7         |1543        |275           |17.822423849643553|\n|12        |1558        |277           |17.77920410783055 |\n|8         |1637        |291           |17.77642028100183 |\n|21        |1563        |276           |17.65834932821497 |\n|20        |1617        |282           |17.439703153988866|\n|18        |1618        |282           |17.428924598269468|\n|23        |1569        |273           |17.39961759082218 |\n|30        |1494        |259           |17.336010709504684|\n|3         |1587        |275           |17.328292375551353|\n|9         |1612        |278           |17.24565756823821 |\n|28        |1676        |289           |17.24343675417661 |\n|22        |1550        |267           |17.225806451612904|\n+----------+------------+--------------+------------------+\nonly showing top 20 rows\n\n\n--- Results for column: DayOfWeek ---\n+---------+------------+--------------+------------------+\n|DayOfWeek|TotalFlights|DelayedFlights|DelayPercentage   |\n+---------+------------+--------------+------------------+\n|5        |7208        |1435          |19.908435072142066|\n|4        |7225        |1347          |18.643598615916954|\n|7        |7284        |1271          |17.44920373421197 |\n|1        |7078        |1228          |17.349533766600736|\n|6        |6269        |1035          |16.50981017706173 |\n|3        |6715        |1063          |15.830230826507819|\n|2        |6638        |1017          |15.32087978306719 |\n+---------+------------+--------------+------------------+\n\n\n\n--- Results for column: Operating_Airline ---\n+-----------------+------------+--------------+------------------+\n|Operating_Airline|TotalFlights|DelayedFlights|DelayPercentage   |\n+-----------------+------------+--------------+------------------+\n|KS               |4           |3             |75.0              |\n|B6               |1936        |495           |25.568181818181817|\n|G4               |763         |194           |25.4259501965924  |\n|F9               |1002        |254           |25.349301397205586|\n|C5               |440         |107           |24.31818181818182 |\n|G7               |456         |109           |23.903508771929825|\n|EV               |511         |119           |23.28767123287671 |\n|CP               |240         |54            |22.5              |\n|AX               |252         |56            |22.22222222222222 |\n|NK               |1434        |284           |19.804741980474198|\n|AA               |5137        |969           |18.86314969826747 |\n|UA               |3965        |730           |18.41109709962169 |\n|YV               |1212        |221           |18.23432343234323 |\n|OH               |1546        |277           |17.917205692108666|\n|WN               |8985        |1573          |17.506956037840844|\n|AS               |1514        |252           |16.6446499339498  |\n|MQ               |1773        |290           |16.356457980823464|\n|YX               |2157        |338           |15.669911914696339|\n|ZW               |599         |93            |15.525876460767945|\n|OO               |5265        |816           |15.498575498575498|\n+-----------------+------------+--------------+------------------+\nonly showing top 20 rows\n\n\n--- Results for column: OriginAirportID ---\n+---------------+------------+--------------+-----------------+\n|OriginAirportID|TotalFlights|DelayedFlights|DelayPercentage  |\n+---------------+------------+--------------+-----------------+\n|12250          |1           |1             |100.0            |\n|14231          |2           |2             |100.0            |\n|10917          |1           |1             |100.0            |\n|12915          |11          |7             |63.63636363636363|\n|13347          |4           |2             |50.0             |\n|11445          |2           |1             |50.0             |\n|14716          |6           |3             |50.0             |\n|14952          |8           |4             |50.0             |\n|14512          |8           |4             |50.0             |\n|13541          |2           |1             |50.0             |\n|10558          |2           |1             |50.0             |\n|11867          |2           |1             |50.0             |\n|12544          |13          |6             |46.15384615384615|\n|15027          |10          |4             |40.0             |\n|10577          |5           |2             |40.0             |\n|13964          |5           |2             |40.0             |\n|11721          |40          |15            |37.5             |\n|11778          |11          |4             |36.36363636363637|\n|15070          |11          |4             |36.36363636363637|\n|14259          |3           |1             |33.33333333333333|\n+---------------+------------+--------------+-----------------+\nonly showing top 20 rows\n\n\n--- Results for column: OriginCityName ---\n+---------------------+------------+--------------+-----------------+\n|OriginCityName       |TotalFlights|DelayedFlights|DelayPercentage  |\n+---------------------+------------+--------------+-----------------+\n|Presque Isle/Houlton |2           |2             |100.0            |\n|Hyannis              |1           |1             |100.0            |\n|Cold Bay             |1           |1             |100.0            |\n|Lake Charles         |11          |7             |63.63636363636363|\n|Concord              |9           |5             |55.55555555555556|\n|Unalaska             |2           |1             |50.0             |\n|Rockford             |8           |4             |50.0             |\n|Martha's Vineyard    |2           |1             |50.0             |\n|Stockton             |6           |3             |50.0             |\n|Hoolehua             |4           |2             |50.0             |\n|Garden City          |2           |1             |50.0             |\n|Scottsbluff          |2           |1             |50.0             |\n|Binghamton           |5           |2             |40.0             |\n|North Bend/Coos Bay  |5           |2             |40.0             |\n|Christiansted        |10          |4             |40.0             |\n|Flint                |40          |15            |37.5             |\n|Newburgh/Poughkeepsie|11          |4             |36.36363636363637|\n|Fort Smith           |11          |4             |36.36363636363637|\n|Elko                 |6           |2             |33.33333333333333|\n|Meridian             |3           |1             |33.33333333333333|\n+---------------------+------------+--------------+-----------------+\nonly showing top 20 rows\n\n\n--- Results for column: OriginStateName ---\n+-------------------+------------+--------------+------------------+\n|OriginStateName    |TotalFlights|DelayedFlights|DelayPercentage   |\n+-------------------+------------+--------------+------------------+\n|U.S. Virgin Islands|37          |11            |29.72972972972973 |\n|Maine              |119         |32            |26.89075630252101 |\n|Puerto Rico        |204         |50            |24.509803921568626|\n|New Jersey         |954         |230           |24.10901467505241 |\n|Louisiana          |507         |113           |22.287968441814595|\n|West Virginia      |43          |9             |20.930232558139537|\n|Colorado           |2176        |452           |20.772058823529413|\n|South Dakota       |99          |20            |20.2020202020202  |\n|Illinois           |2859        |575           |20.111927247289263|\n|Maryland           |678         |136           |20.058997050147493|\n|Florida            |4039        |801           |19.831641495419657|\n|New York           |2220        |431           |19.414414414414413|\n|Connecticut        |174         |33            |18.96551724137931 |\n|Texas              |5077        |936           |18.436084301753002|\n|New Mexico         |190         |35            |18.421052631578945|\n|Missouri           |818         |148           |18.09290953545232 |\n|Massachusetts      |892         |159           |17.825112107623315|\n|Virginia           |1785        |314           |17.591036414565828|\n|Tennessee          |870         |153           |17.586206896551722|\n|North Carolina     |2158        |377           |17.46987951807229 |\n+-------------------+------------+--------------+------------------+\nonly showing top 20 rows\n\n\n--- Results for column: DestAirportID ---\n+-------------+------------+--------------+------------------+\n|DestAirportID|TotalFlights|DelayedFlights|DelayPercentage   |\n+-------------+------------+--------------+------------------+\n|11445        |2           |2             |100.0             |\n|11233        |1           |1             |100.0             |\n|10165        |1           |1             |100.0             |\n|11525        |1           |1             |100.0             |\n|11315        |4           |3             |75.0              |\n|11274        |3           |2             |66.66666666666666 |\n|14905        |3           |2             |66.66666666666666 |\n|14254        |2           |1             |50.0              |\n|10154        |4           |2             |50.0              |\n|13344        |2           |1             |50.0              |\n|11283        |4           |2             |50.0              |\n|11587        |4           |2             |50.0              |\n|15897        |2           |1             |50.0              |\n|10631        |4           |2             |50.0              |\n|14109        |4           |2             |50.0              |\n|10676        |2           |1             |50.0              |\n|14082        |48          |22            |45.83333333333333 |\n|14520        |9           |4             |44.44444444444444 |\n|10146        |7           |3             |42.857142857142854|\n|11898        |12          |5             |41.66666666666667 |\n+-------------+------------+--------------+------------------+\nonly showing top 20 rows\n\n\n--- Results for column: DestCityName ---\n+------------------+------------+--------------+-----------------+\n|DestCityName      |TotalFlights|DelayedFlights|DelayPercentage  |\n+------------------+------------+--------------+-----------------+\n|Unalaska          |2           |2             |100.0            |\n|Cheyenne          |1           |1             |100.0            |\n|Adak Island       |1           |1             |100.0            |\n|Elko              |1           |1             |100.0            |\n|Dickinson         |4           |3             |75.0             |\n|Dubuque           |3           |2             |66.66666666666666|\n|Santa Maria       |3           |2             |66.66666666666666|\n|Ponce             |2           |1             |50.0             |\n|Nantucket         |4           |2             |50.0             |\n|Bemidji           |4           |2             |50.0             |\n|Belleville        |2           |1             |50.0             |\n|Escanaba          |4           |2             |50.0             |\n|Muskegon          |2           |1             |50.0             |\n|Hattiesburg/Laurel|4           |2             |50.0             |\n|West Yellowstone  |2           |1             |50.0             |\n|Dodge City        |4           |2             |50.0             |\n|Punta Gorda       |48          |22            |45.83333333333333|\n|Rhinelander       |9           |4             |44.44444444444444|\n|Grand Forks       |12          |5             |41.66666666666667|\n|CONCORD           |5           |2             |40.0             |\n+------------------+------------+--------------+-----------------+\nonly showing top 20 rows\n\n\n--- Results for column: DestState ---\n+---------+------------+--------------+------------------+\n|DestState|TotalFlights|DelayedFlights|DelayPercentage   |\n+---------+------------+--------------+------------------+\n|ME       |132         |36            |27.27272727272727 |\n|NJ       |976         |257           |26.331967213114755|\n|VT       |50          |12            |24.0              |\n|CT       |155         |36            |23.225806451612904|\n|LA       |490         |110           |22.448979591836736|\n|PR       |222         |49            |22.07207207207207 |\n|NH       |64          |14            |21.875            |\n|MA       |863         |187           |21.668597914252608|\n|OK       |259         |55            |21.235521235521233|\n|FL       |3925        |822           |20.94267515923567 |\n|NY       |2204        |459           |20.825771324863883|\n|KS       |123         |25            |20.32520325203252 |\n|AL       |248         |50            |20.161290322580644|\n|WV       |52          |10            |19.230769230769234|\n|RI       |115         |22            |19.130434782608695|\n|IN       |420         |80            |19.047619047619047|\n|WI       |424         |79            |18.632075471698112|\n|TX       |5072        |927           |18.276813880126184|\n|TN       |891         |160           |17.957351290684624|\n|ND       |123         |22            |17.88617886178862 |\n+---------+------------+--------------+------------------+\nonly showing top 20 rows\n\n\n--- Results for column: DestStateName ---\n+-------------+------------+--------------+------------------+\n|DestStateName|TotalFlights|DelayedFlights|DelayPercentage   |\n+-------------+------------+--------------+------------------+\n|Maine        |132         |36            |27.27272727272727 |\n|New Jersey   |976         |257           |26.331967213114755|\n|Vermont      |50          |12            |24.0              |\n|Connecticut  |155         |36            |23.225806451612904|\n|Louisiana    |490         |110           |22.448979591836736|\n|Puerto Rico  |222         |49            |22.07207207207207 |\n|New Hampshire|64          |14            |21.875            |\n|Massachusetts|863         |187           |21.668597914252608|\n|Oklahoma     |259         |55            |21.235521235521233|\n|Florida      |3925        |822           |20.94267515923567 |\n|New York     |2204        |459           |20.825771324863883|\n|Kansas       |123         |25            |20.32520325203252 |\n|Alabama      |248         |50            |20.161290322580644|\n|West Virginia|52          |10            |19.230769230769234|\n|Rhode Island |115         |22            |19.130434782608695|\n|Indiana      |420         |80            |19.047619047619047|\n|Wisconsin    |424         |79            |18.632075471698112|\n|Texas        |5072        |927           |18.276813880126184|\n|Tennessee    |891         |160           |17.957351290684624|\n|North Dakota |123         |22            |17.88617886178862 |\n+-------------+------------+--------------+------------------+\nonly showing top 20 rows\n\n\n--- Results for column: DistanceGroup ---\n+-------------+------------+--------------+------------------+\n|DistanceGroup|TotalFlights|DelayedFlights|DelayPercentage   |\n+-------------+------------+--------------+------------------+\n|7            |2046        |394           |19.257086999022484|\n|5            |5188        |998           |19.236700077101002|\n|11           |874         |168           |19.221967963386728|\n|4            |7381        |1376          |18.642460371223414|\n|9            |651         |119           |18.27956989247312 |\n|10           |1161        |204           |17.571059431524546|\n|3            |9439        |1644          |17.41709926899036 |\n|8            |957         |156           |16.30094043887147 |\n|2            |11964       |1930          |16.131728518890004|\n|1            |6803        |1097          |16.125238865206526|\n|6            |1953        |310           |15.873015873015872|\n+-------------+------------+--------------+------------------+\n\n\n\n--- Results for column: Airline_enc ---\n+-------------------+------------+--------------+------------------+\n|Airline_enc        |TotalFlights|DelayedFlights|DelayPercentage   |\n+-------------------+------------+--------------+------------------+\n|0.75               |4           |3             |75.0              |\n|0.2556818181818182 |1936        |495           |25.568181818181817|\n|0.254259501965924  |763         |194           |25.4259501965924  |\n|0.25349301397205587|1002        |254           |25.349301397205586|\n|0.2431818181818182 |440         |107           |24.31818181818182 |\n|0.23903508771929824|456         |109           |23.903508771929825|\n|0.2328767123287671 |511         |119           |23.28767123287671 |\n|0.225              |240         |54            |22.5              |\n|0.2222222222222222 |252         |56            |22.22222222222222 |\n|0.19804741980474197|1434        |284           |19.804741980474198|\n|0.1886314969826747 |5137        |969           |18.86314969826747 |\n|0.1841109709962169 |3965        |730           |18.41109709962169 |\n|0.18234323432343233|1212        |221           |18.23432343234323 |\n|0.17917205692108668|1546        |277           |17.917205692108666|\n|0.17506956037840846|8985        |1573          |17.506956037840844|\n|0.166446499339498  |1514        |252           |16.6446499339498  |\n|0.16356457980823463|1773        |290           |16.356457980823464|\n|0.15669911914696338|2157        |338           |15.669911914696339|\n|0.15525876460767946|599         |93            |15.525876460767945|\n|0.15498575498575498|5265        |816           |15.498575498575498|\n+-------------------+------------+--------------+------------------+\nonly showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, count, when\n",
    "\n",
    "# List of columns to exclude\n",
    "exclude_columns = [\n",
    "    \"ArrivalDelayGroups\", \"ArrDel15\", \"ArrDelay\", \"CRSArrTime\", \"TaxiIn\", \n",
    "    \"WheelsOn\", \"WheelsOff\", \"TaxiOut\", \"DepartureDelayGroups\", \"DepDel15\", \n",
    "    \"ActualElapsedTime\", \"CRSElapsedTime\", \"AirTime\", \"ArrDelayMinutes\", \n",
    "    \"ArrTime\", \"DepDelay\", \"DepDelayMinutes\", \"DepTime\", \"CRSDepTime\", \"FlightDate\"\n",
    "]\n",
    "\n",
    "# Loop through all columns except the target 'Delayed' and the ones to be excluded\n",
    "for group_col in [c for c in df.columns if c != \"Delayed\" and c not in exclude_columns]:\n",
    "    print(f\"\\n\\n--- Results for column: {group_col} ---\")\n",
    "    \n",
    "    # Group by the column, then compute total flights and delayed flights for each group value\n",
    "    df_result = (\n",
    "        df.groupBy(group_col)\n",
    "          .agg(\n",
    "              count(\"*\").alias(\"TotalFlights\"),\n",
    "              count(when(col(\"Delayed\") == 1, True)).alias(\"DelayedFlights\")\n",
    "          )\n",
    "          .withColumn(\"DelayPercentage\", (col(\"DelayedFlights\") / col(\"TotalFlights\")) * 100)\n",
    "    )\n",
    "    \n",
    "    # Order the results by DelayPercentage descending (so groups with highest delay percentage appear first)\n",
    "    df_result_ordered = df_result.orderBy(col(\"DelayPercentage\").desc())\n",
    "    \n",
    "    # Show the result; you can adjust the number of rows to show as needed\n",
    "    df_result_ordered.show(truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a91eb22-079f-4e89-b9a6-3a7c9538b3f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Define a function to categorize time into time group with numeric values\n",
    "def time_group_udf(time_column):\n",
    "    return F.when((time_column >= 600) & (time_column < 1200), 1) \\\n",
    "           .when((time_column >= 1200) & (time_column < 1800), 2) \\\n",
    "           .when((time_column >= 1800) & (time_column < 2100), 3) \\\n",
    "           .otherwise(4)\n",
    "\n",
    "# Apply the function to categorize the time columns\n",
    "df_with_time_groups = df.withColumn(\"CRSDepTime_TimeOfDay\", time_group_udf(F.col(\"CRSDepTime\"))) \\\n",
    "                        .withColumn(\"DepTime_TimeOfDay\", time_group_udf(F.col(\"DepTime\"))) \\\n",
    "                        .withColumn(\"ArrTime_TimeOfDay\", time_group_udf(F.col(\"ArrTime\"))) \\\n",
    "                        .withColumn(\"WheelsOff_TimeOfDay\", time_group_udf(F.col(\"WheelsOff\"))) \\\n",
    "                        .withColumn(\"WheelsOn_TimeOfDay\", time_group_udf(F.col(\"WheelsOn\"))) \\\n",
    "                        .withColumn(\"TaxiOut_TimeOfDay\", time_group_udf(F.col(\"TaxiOut\"))) \\\n",
    "                        .withColumn(\"TaxiIn_TimeOfDay\", time_group_udf(F.col(\"TaxiIn\")))\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "\n",
    "# Save the DataFrame to a CSV file with a valid path\n",
    "# Save the DataFrame to DBFS\n",
    "df_with_time_groups.write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"/dbfs/mnt/your_mount_point/flight_times_grouped.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ea3f549-2672-4fda-b7f3-8d68fd03d426",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+\n|DistanceGroup|Distance|\n+-------------+--------+\n|            1|   102.0|\n|            2|   496.0|\n|            3|   571.0|\n|            4|   990.0|\n|            6|  1294.0|\n|            2|   377.0|\n|            4|   912.0|\n|           10|  2434.0|\n|            5|  1020.0|\n|            1|   235.0|\n|            2|   351.0|\n|            8|  1770.0|\n|            1|   236.0|\n|            2|   296.0|\n|            4|   895.0|\n|            1|   232.0|\n|            1|   192.0|\n|            4|   950.0|\n|            2|   402.0|\n|            3|   594.0|\n|            2|   313.0|\n|            4|   849.0|\n|            2|   399.0|\n|            3|   562.0|\n|            3|   599.0|\n|            2|   304.0|\n|            2|   373.0|\n|            3|   666.0|\n|            3|   626.0|\n|            3|   526.0|\n|            5|  1074.0|\n|            3|   679.0|\n|            4|   834.0|\n|            5|  1134.0|\n|            4|   904.0|\n|            3|   627.0|\n|            2|   331.0|\n|            2|   463.0|\n|            7|  1521.0|\n|            1|   192.0|\n|            2|   326.0|\n|            3|   577.0|\n|            1|    95.0|\n|            2|   406.0|\n|           11|  2762.0|\n|           10|  2405.0|\n|            5|  1024.0|\n|            3|   731.0|\n|            4|   936.0|\n|            4|   972.0|\n|            4|   933.0|\n|           10|  2419.0|\n|            7|  1703.0|\n|            4|   844.0|\n|            5|  1062.0|\n|            3|   645.0|\n|            4|   794.0|\n|            4|   895.0|\n|            9|  2133.0|\n|            4|   814.0|\n|            6|  1444.0|\n|            5|  1050.0|\n|            4|   825.0|\n|            3|   566.0|\n|            1|   224.0|\n|            1|   191.0|\n|            5|  1065.0|\n|            2|   496.0|\n|            6|  1476.0|\n|            2|   388.0|\n|            3|   572.0|\n|            3|   526.0|\n|            4|   762.0|\n|            2|   488.0|\n|            2|   343.0|\n|            6|  1379.0|\n|            1|   113.0|\n|            4|   967.0|\n|            1|    86.0|\n|            4|   990.0|\n|            1|   125.0|\n|            2|   331.0|\n|            3|   735.0|\n|            1|   221.0|\n|           11|  2611.0|\n|            5|  1047.0|\n|            2|   379.0|\n|            2|   388.0|\n|            2|   308.0|\n|            7|  1587.0|\n|            5|  1145.0|\n|            2|   446.0|\n|            2|   480.0|\n|            7|  1590.0|\n|            3|   719.0|\n|            2|   404.0|\n|            4|   977.0|\n|            9|  2039.0|\n|            3|   596.0|\n|            1|   143.0|\n|            3|   588.0|\n|            2|   326.0|\n|            2|   349.0|\n|            2|   343.0|\n|            6|  1300.0|\n|            2|   373.0|\n|            5|  1119.0|\n|            4|   930.0|\n|            6|  1405.0|\n|            5|  1072.0|\n|            3|   588.0|\n|            1|   108.0|\n|            2|   379.0|\n|            8|  1990.0|\n|            3|   547.0|\n|            3|   585.0|\n|            5|  1066.0|\n|            7|  1605.0|\n|            7|  1500.0|\n|           10|  2454.0|\n|            3|   515.0|\n|            1|   110.0|\n|            1|    83.0|\n|            3|   501.0|\n|            2|   324.0|\n|            5|  1096.0|\n|            1|   225.0|\n|            1|   135.0|\n|            7|  1599.0|\n|            2|   431.0|\n|            1|   158.0|\n|            2|   335.0|\n|           11|  2537.0|\n|            1|   224.0|\n|            8|  1754.0|\n|            2|   308.0|\n|            2|   446.0|\n|            4|   834.0|\n|            1|   177.0|\n|            1|   201.0|\n|            2|   412.0|\n|           10|  2398.0|\n|            2|   258.0|\n|            4|   822.0|\n|            5|  1148.0|\n|            1|   140.0|\n|            3|   534.0|\n|            1|   248.0|\n|            3|   657.0|\n|            1|   224.0|\n+-------------+--------+\nonly showing top 150 rows\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\n1: Distance between 0 and 250 miles\\n2: Distance between 250 and 500 miles\\n3: Distance between 500 and 750 miles\\n4: Distance between 750 and 1000 miles\\n5: Distance between 1000 and 1500 miles\\n6: Distance between 1500 and 2000 miles\\nAnd so on...\\n\\n'"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming your DataFrame is named 'df'\n",
    "df_selected = df.select(\"DistanceGroup\", \"Distance\")\n",
    "\n",
    "# Show the result\n",
    "df_selected.show(150)\n",
    "\n",
    "\n",
    "## The range is like \n",
    "\"\"\"\n",
    "1: Distance between 0 and 250 miles\n",
    "2: Distance between 250 and 500 miles\n",
    "3: Distance between 500 and 750 miles\n",
    "4: Distance between 750 and 1000 miles\n",
    "5: Distance between 1000 and 1500 miles\n",
    "6: Distance between 1500 and 2000 miles\n",
    "And so on...\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c116da8-7074-4ad6-9fc6-0050cf0b94a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+--------------+------------------+\n|TimeOfDay|TotalFlights|DelayedFlights|DelayPercentage   |\n+---------+------------+--------------+------------------+\n|Evening  |7697        |1794          |23.307782252825778|\n|Afternoon|17598       |3486          |19.809069212410503|\n|Night    |3993        |745           |18.657650889055848|\n|Morning  |19129       |2371          |12.394793245857077|\n+---------+------------+--------------+------------------+\n\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nMorning: 6 AM to 12 PM\\nAfternoon: 12 PM to 6 PM\\nEvening: 6 PM to 9 PM\\nNight: 9 PM to 6 AM\\n\\n'"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Define a function to categorize departure times into groups (morning, afternoon, evening, night)\n",
    "def categorize_time_of_day(dep_time):\n",
    "    # Convert CRSDepTime (military format) to hour\n",
    "    hour = dep_time // 100  # Extract hour (dividing by 100)\n",
    "    \n",
    "    # Categorize based on hour\n",
    "    if 6 <= hour < 12:\n",
    "        return \"Morning\"\n",
    "    elif 12 <= hour < 18:\n",
    "        return \"Afternoon\"\n",
    "    elif 18 <= hour < 21:\n",
    "        return \"Evening\"\n",
    "    else:\n",
    "        return \"Night\"\n",
    "\n",
    "# UDF to apply the time categorization\n",
    "def time_of_day_udf(dep_time):\n",
    "    return categorize_time_of_day(dep_time)\n",
    "\n",
    "# Register the UDF\n",
    "time_of_day = F.udf(time_of_day_udf)\n",
    "\n",
    "# Add the 'TimeOfDay' column based on the CRSDepTime or DepTime column (whichever you prefer)\n",
    "df_with_time_of_day = df.withColumn('TimeOfDay', time_of_day(F.col('CRSDepTime')))\n",
    "\n",
    "# Group by the new 'TimeOfDay' column and calculate delay percentage\n",
    "df_result = (\n",
    "    df_with_time_of_day.groupBy('TimeOfDay')\n",
    "    .agg(\n",
    "        F.count('*').alias('TotalFlights'),\n",
    "        F.count(F.when(F.col('Delayed') == 1, 1)).alias('DelayedFlights')\n",
    "    )\n",
    "    .withColumn('DelayPercentage', (F.col('DelayedFlights') / F.col('TotalFlights')) * 100)\n",
    "    .orderBy(F.col('DelayPercentage').desc())\n",
    ")\n",
    "\n",
    "# Show the result\n",
    "df_result.show(truncate=False)\n",
    "\n",
    "########\n",
    "\"\"\"\n",
    "Morning: 6 AM to 12 PM\n",
    "Afternoon: 12 PM to 6 PM\n",
    "Evening: 6 PM to 9 PM\n",
    "Night: 9 PM to 6 AM\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19c88eea-4654-483c-9bf8-145743934088",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nResults for CRSDepTime Time of Day:\n+--------------------+------------+--------------+------------------+\n|CRSDepTime_TimeOfDay|TotalFlights|DelayedFlights|DelayPercentage   |\n+--------------------+------------+--------------+------------------+\n|Evening             |7697        |1794          |23.307782252825778|\n|Afternoon           |17598       |3486          |19.809069212410503|\n|Night               |3993        |745           |18.657650889055848|\n|Morning             |19129       |2371          |12.394793245857077|\n+--------------------+------------+--------------+------------------+\n\n\nResults for DepTime Time of Day:\n+-----------------+------------+--------------+------------------+\n|DepTime_TimeOfDay|TotalFlights|DelayedFlights|DelayPercentage   |\n+-----------------+------------+--------------+------------------+\n|Evening          |7690        |1913          |24.876462938881662|\n|Night            |5358        |1260          |23.516237402015676|\n|Afternoon        |17297       |3194          |18.46562987801353 |\n|Morning          |18072       |2029          |11.22731297034086 |\n+-----------------+------------+--------------+------------------+\n\n\nResults for TaxiOut Time of Day:\n+-----------------+------------+--------------+-----------------+\n|TaxiOut_TimeOfDay|TotalFlights|DelayedFlights|DelayPercentage  |\n+-----------------+------------+--------------+-----------------+\n|Night            |48417       |8396          |17.34101658508375|\n+-----------------+------------+--------------+-----------------+\n\n\nResults for WheelsOff Time of Day:\n+-------------------+------------+--------------+------------------+\n|WheelsOff_TimeOfDay|TotalFlights|DelayedFlights|DelayPercentage   |\n+-------------------+------------+--------------+------------------+\n|Night              |4901        |1402          |28.606406855743728|\n|Evening            |8035        |1979          |24.62974486621033 |\n|Afternoon          |17185       |3107          |18.07972068664533 |\n|Morning            |18296       |1908          |10.428508963707914|\n+-------------------+------------+--------------+------------------+\n\n\nResults for WheelsOn Time of Day:\n+------------------+------------+--------------+------------------+\n|WheelsOn_TimeOfDay|TotalFlights|DelayedFlights|DelayPercentage   |\n+------------------+------------+--------------+------------------+\n|Night             |8338        |2512          |30.127128807867592|\n|Evening           |8676        |1858          |21.415398801290916|\n|Afternoon         |17504       |2781          |15.887797074954296|\n|Morning           |13899       |1245          |8.957478955320527 |\n+------------------+------------+--------------+------------------+\n\n\nResults for TaxiIn Time of Day:\n+----------------+------------+--------------+-----------------+\n|TaxiIn_TimeOfDay|TotalFlights|DelayedFlights|DelayPercentage  |\n+----------------+------------+--------------+-----------------+\n|Night           |48417       |8396          |17.34101658508375|\n+----------------+------------+--------------+-----------------+\n\n\nResults for ArrTime Time of Day:\n+-----------------+------------+--------------+------------------+\n|ArrTime_TimeOfDay|TotalFlights|DelayedFlights|DelayPercentage   |\n+-----------------+------------+--------------+------------------+\n|Night            |8611        |2603          |30.228777145511554|\n|Evening          |8715        |1848          |21.204819277108435|\n|Afternoon        |17456       |2737          |15.67942254812099 |\n|Morning          |13635       |1208          |8.85955262192886  |\n+-----------------+------------+--------------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Define a function to categorize any time column into periods of the day (morning, afternoon, evening, night)\n",
    "def categorize_time_of_day(time_col):\n",
    "    # Convert time to hour\n",
    "    hour = time_col // 100  # Extract hour (by dividing by 100)\n",
    "    \n",
    "    # Categorize based on hour\n",
    "    if 6 <= hour < 12:\n",
    "        return \"Morning\"\n",
    "    elif 12 <= hour < 18:\n",
    "        return \"Afternoon\"\n",
    "    elif 18 <= hour < 21:\n",
    "        return \"Evening\"\n",
    "    else:\n",
    "        return \"Night\"\n",
    "\n",
    "# UDF for categorization\n",
    "def time_of_day_udf(time_col):\n",
    "    return categorize_time_of_day(time_col)\n",
    "\n",
    "# Register the UDF\n",
    "time_of_day = F.udf(time_of_day_udf)\n",
    "\n",
    "# List of columns to categorize\n",
    "time_columns = [\n",
    "    'CRSDepTime', 'DepTime', 'TaxiOut', 'WheelsOff', 'WheelsOn', 'TaxiIn', 'ArrTime'\n",
    "]\n",
    "\n",
    "# Loop through each time column and categorize the time of day\n",
    "for col_name in time_columns:\n",
    "    df_with_time_of_day = df.withColumn(f'{col_name}_TimeOfDay', time_of_day(F.col(col_name)))\n",
    "    \n",
    "    # Group by the new TimeOfDay column and calculate delay percentage for each time column\n",
    "    df_result = (\n",
    "        df_with_time_of_day.groupBy(f'{col_name}_TimeOfDay')\n",
    "        .agg(\n",
    "            F.count('*').alias('TotalFlights'),\n",
    "            F.count(F.when(F.col('Delayed') == 1, 1)).alias('DelayedFlights')\n",
    "        )\n",
    "        .withColumn('DelayPercentage', (F.col('DelayedFlights') / F.col('TotalFlights')) * 100)\n",
    "        .orderBy(F.col('DelayPercentage').desc())\n",
    "    )\n",
    "    \n",
    "    # Show the result\n",
    "    print(f\"\\nResults for {col_name} Time of Day:\")\n",
    "    df_result.show(truncate=False)\n",
    "\n",
    "\n",
    "\n",
    "### ANALYSIS -->   it seems like evening departures and night arrivals are more prone to delays.\n",
    "### Evening departures could be affected by increased air traffic during peak evening hours, with delays possibly compounding as the day progresses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f96d14a5-eaf0-4167-a7e0-08876c4b0652",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+----+-----+---------+-------+--------+----------+-------+-------+\n|        Airline_enc|Origin|Dest|Month|DayOfWeek|TaxiOut|Distance|CRSDepTime|DepTime|Delayed|\n+-------------------+------+----+-----+---------+-------+--------+----------+-------+-------+\n|        0.111328125|   LIH| HNL|    3|        6|    6.0|   102.0|      1900| 1855.0|      0|\n| 0.1278928136419001|   BOS| PIT|    4|        1|   22.0|   496.0|      2055| 2111.0|      1|\n|0.17506956037840846|   HOU| ECP|    2|        5|   16.0|   571.0|      1650| 1728.0|      1|\n|0.17506956037840846|   MDW| MCO|   12|        5|    9.0|   990.0|      1845| 1851.0|      0|\n|0.17506956037840846|   LAS| LIT|    6|        7|    7.0|  1294.0|      1200| 1201.0|      0|\n|0.17506956037840846|   BWI| MHT|    8|        2|    8.0|   377.0|      1055| 1054.0|      0|\n|0.18234323432343233|   IAH| CLT|    2|        5|   25.0|   912.0|      1025| 1022.0|      0|\n| 0.1841109709962169|   EWR| PDX|   11|        3|   36.0|  2434.0|       845|  839.0|      0|\n| 0.1886314969826747|   PHX| GEG|    9|        7|   40.0|  1020.0|      1012| 1002.0|      0|\n|0.12481831395348837|   DTW| ORD|   12|        5|   17.0|   235.0|      1558| 1555.0|      0|\n| 0.2328767123287671|   JAN| IAH|    9|        3|   10.0|   351.0|      1200| 1149.0|      0|\n|  0.166446499339498|   SEA| AUS|    9|        7|   31.0|  1770.0|      1740| 1735.0|      0|\n| 0.1886314969826747|   LAS| LAX|   11|        2|   12.0|   236.0|       725|  949.0|      1|\n|0.17506956037840846|   BUR| SJC|    5|        2|   10.0|   296.0|      1700| 1656.0|      0|\n| 0.1841109709962169|   CLE| MCO|    6|        7|   10.0|   895.0|       900|  852.0|      0|\n| 0.1278928136419001|   DSM| MSP|    9|        5|   20.0|   232.0|      1549| 1543.0|      0|\n|0.15498575498575498|   SFO| RNO|    8|        5|   29.0|   192.0|      1300| 1634.0|      1|\n|0.12481831395348837|   MCO| LGA|   12|        4|   13.0|   950.0|       610|  605.0|      0|\n| 0.2556818181818182|   LGB| RNO|   10|        7|   11.0|   402.0|      1138| 1132.0|      0|\n|0.16356457980823463|   ORD| MDT|   11|        1|   14.0|   594.0|       825|  825.0|      0|\n+-------------------+------+----+-----+---------+-------+--------+----------+-------+-------+\nonly showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "#Select required columns\n",
    "selected_columns = [\"Airline_enc\", \"Origin\", \"Dest\", \"Month\", \"DayOfWeek\", \"TaxiOut\", \"Distance\", \"CRSDepTime\", \"DepTime\", \"Delayed\"]\n",
    "df_selected = df.select(selected_columns)\n",
    "\n",
    "# Show the filtered data\n",
    "df_selected.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "963f6e6a-417e-4482-83dd-321fa2107ee1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+-------------------+-----+---------+-------+--------+----------+-------+-------+\n|   Airline_enc_mean|         Origin_enc|           Dest_enc|Month|DayOfWeek|TaxiOut|Distance|CRSDepTime|DepTime|Delayed|\n+-------------------+-------------------+-------------------+-----+---------+-------+--------+----------+-------+-------+\n|0.17506956037840846|0.20810810810810812| 0.1568627450980392|    2|        5|   16.0|   571.0|      1650| 1728.0|      1|\n|0.17506956037840846|0.20433996383363473|0.23732251521298176|   12|        5|    9.0|   990.0|      1845| 1851.0|      0|\n|0.17506956037840846|0.17280701754385966| 0.1956521739130435|    6|        7|    7.0|  1294.0|      1200| 1201.0|      0|\n|0.17506956037840846| 0.1996996996996997| 0.2222222222222222|    8|        2|    8.0|   377.0|      1055| 1054.0|      0|\n|0.17506956037840846|0.11731843575418995|0.15365239294710328|    5|        2|   10.0|   296.0|      1700| 1656.0|      0|\n| 0.1841109709962169| 0.2429193899782135|0.16545454545454547|   11|        3|   36.0|  2434.0|       845|  839.0|      0|\n| 0.1841109709962169|0.15753424657534246|0.23732251521298176|    6|        7|   10.0|   895.0|       900|  852.0|      0|\n|0.16356457980823463|0.20110446387482742|0.20588235294117646|   11|        1|   14.0|   594.0|       825|  825.0|      0|\n| 0.2556818181818182| 0.1452991452991453|0.09859154929577464|   10|        7|   11.0|   402.0|      1138| 1132.0|      0|\n|        0.111328125|0.05194805194805195|0.13314447592067988|    3|        6|    6.0|   102.0|      1900| 1855.0|      0|\n|  0.166446499339498|0.15560821484992102|0.19153225806451613|    9|        7|   31.0|  1770.0|      1740| 1735.0|      0|\n|0.18234323432343233|0.17892976588628762|0.13998613998613998|    2|        5|   25.0|   912.0|      1025| 1022.0|      0|\n| 0.2328767123287671|0.12727272727272726|0.17346053772766695|    9|        3|   10.0|   351.0|      1200| 1149.0|      0|\n| 0.1278928136419001|0.17572254335260115|0.16393442622950818|    4|        1|   22.0|   496.0|      2055| 2111.0|      1|\n| 0.1278928136419001|0.13821138211382114|0.12247191011235956|    9|        5|   20.0|   232.0|      1549| 1543.0|      0|\n|0.12481831395348837| 0.1427061310782241|0.16460176991150444|   12|        5|   17.0|   235.0|      1558| 1555.0|      0|\n|0.12481831395348837|0.20456802383316783| 0.2079806529625151|   12|        4|   13.0|   950.0|       610|  605.0|      0|\n|0.15498575498575498|0.16555023923444975|0.09859154929577464|    8|        5|   29.0|   192.0|      1300| 1634.0|      1|\n|0.15498575498575498|0.20814013395157135|                0.0|    3|        6|   16.0|   313.0|      1129| 1117.0|      0|\n| 0.1886314969826747|0.16331658291457288|               0.14|    9|        7|   40.0|  1020.0|      1012| 1002.0|      0|\n+-------------------+-------------------+-------------------+-----+---------+-------+--------+----------+-------+-------+\nonly showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Step 1: Select required columns\n",
    "selected_columns = [\"Airline_enc\", \"Origin\", \"Dest\", \"Month\", \"DayOfWeek\", \"TaxiOut\", \"Distance\", \"CRSDepTime\", \"DepTime\", \"Delayed\"]\n",
    "df_selected = df.select(selected_columns)\n",
    "\n",
    "# Step 2: Target Encoding for 'Airline_enc'\n",
    "airline_target_mean = df_selected.groupBy(\"Airline_enc\").agg(F.mean(\"Delayed\").alias(\"Airline_enc_mean\"))\n",
    "df_with_airline_encoded = df_selected.join(airline_target_mean, on=\"Airline_enc\", how=\"left\").drop(\"Airline_enc\")\n",
    "\n",
    "# Step 3: Target Encoding for 'Origin'\n",
    "origin_target_mean = df_selected.groupBy(\"Origin\").agg(F.mean(\"Delayed\").alias(\"Origin_enc\"))\n",
    "df_with_airline_encoded_origin = df_with_airline_encoded.join(origin_target_mean, on=\"Origin\", how=\"left\").drop(\"Origin\")\n",
    "\n",
    "# Step 4: Target Encoding for 'Dest'\n",
    "dest_target_mean = df_selected.groupBy(\"Dest\").agg(F.mean(\"Delayed\").alias(\"Dest_enc\"))\n",
    "df_final = df_with_airline_encoded_origin.join(dest_target_mean, on=\"Dest\", how=\"left\").drop(\"Dest\")\n",
    "\n",
    "# Step 5: Reorder columns\n",
    "df_final = df_final.select(\"Airline_enc_mean\", \"Origin_enc\", \"Dest_enc\", \"Month\", \"DayOfWeek\", \"TaxiOut\", \"Distance\", \"CRSDepTime\", \"DepTime\", \"Delayed\")\n",
    "\n",
    "# Show the final DataFrame with target encoded columns\n",
    "df_final.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd4a50a6-2238-4c63-8e37-acd316f313d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "497ef322-9db0-4428-9540-8b77c5b6aa7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Assembling all features into a single vector column\n",
    "feature_columns = [\"Airline_enc_mean\", \"Origin_enc\", \"Dest_enc\", \"Month\", \"DayOfWeek\", \n",
    "                   \"TaxiOut\", \"Distance\", \"CRSDepTime\", \"DepTime\"]  # Update based on your actual column names\n",
    "\n",
    "\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3bf57f6-126e-4ed3-9f4d-48d180139f91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Splitting the data into training and testing sets\n",
    "# Splitting the data into training and test sets\n",
    "train_data, test_data = df_final.randomSplit([0.8, 0.2], seed=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a18f7cf3-6408-408d-81dc-011fce0c1236",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initializing RandomForestClassifier\n",
    "rf = RandomForestClassifier(labelCol=\"Delayed\", featuresCol=\"features\", numTrees=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9c77957-fd87-400c-af14-183c23612379",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7455250adb04fcea1e67efd97a72c6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41b5d3271688437cbf83e99f4f72bb8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Creating the pipeline\n",
    "pipeline = Pipeline(stages=[assembler, rf])\n",
    "\n",
    "# Fitting the model to the training data\n",
    "model = pipeline.fit(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8edb736f-a8f3-4d0f-af2c-3c4817bd792c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Making predictions on the test set\n",
    "predictions = model.transform(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a95f090-a406-400b-9083-3fa43dfe0a59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.8191175458440383\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"Delayed\")\n",
    "auc = evaluator.evaluate(predictions)\n",
    "print(f\"AUC: {auc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6b258e9-c300-432a-bfcb-94465e98439a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+--------------------+\n|Delayed|prediction|         probability|\n+-------+----------+--------------------+\n|      0|       0.0|[0.89132415056733...|\n|      0|       0.0|[0.88476266812767...|\n|      1|       0.0|[0.71501254202346...|\n|      0|       0.0|[0.90266858529998...|\n|      0|       0.0|[0.88044688681661...|\n|      0|       0.0|[0.90113465513637...|\n|      0|       0.0|[0.85528519153286...|\n|      0|       0.0|[0.80666834921607...|\n|      0|       0.0|[0.88750001154620...|\n|      0|       0.0|[0.90536339467467...|\n|      0|       0.0|[0.88772168887265...|\n|      0|       0.0|[0.88553836217706...|\n|      0|       0.0|[0.85102759045331...|\n|      0|       0.0|[0.91200705327646...|\n|      0|       0.0|[0.90377717310646...|\n|      0|       0.0|[0.90239264902542...|\n|      0|       0.0|[0.88238247607594...|\n|      0|       0.0|[0.85588031844992...|\n|      0|       0.0|[0.90400785823559...|\n|      0|       0.0|[0.83961955843917...|\n|      0|       0.0|[0.91483608563861...|\n|      0|       0.0|[0.91119013634527...|\n|      0|       0.0|[0.85428676452865...|\n|      0|       0.0|[0.85714269684963...|\n|      1|       0.0|[0.85841708035122...|\n|      0|       0.0|[0.91024464515144...|\n|      0|       0.0|[0.85198243332992...|\n|      0|       0.0|[0.85349077526546...|\n|      0|       0.0|[0.88951859316061...|\n|      0|       0.0|[0.82094153897668...|\n|      1|       0.0|[0.78276903778865...|\n|      0|       0.0|[0.88102693963686...|\n|      0|       0.0|[0.85974707127187...|\n|      0|       0.0|[0.83882356917598...|\n|      0|       0.0|[0.90519116665022...|\n|      0|       0.0|[0.89524411477069...|\n|      0|       0.0|[0.90706906199191...|\n|      0|       0.0|[0.90694260441889...|\n|      0|       0.0|[0.89819607388427...|\n|      0|       0.0|[0.88341026324675...|\n|      0|       0.0|[0.89396405515910...|\n|      0|       0.0|[0.89656159049529...|\n|      0|       0.0|[0.83495922123196...|\n|      0|       0.0|[0.85766687892353...|\n|      0|       0.0|[0.90626156223239...|\n|      1|       0.0|[0.74007623376044...|\n|      0|       0.0|[0.88641519298722...|\n|      0|       0.0|[0.91230159862197...|\n|      0|       0.0|[0.85292306503053...|\n|      0|       0.0|[0.91133237387488...|\n|      0|       0.0|[0.91133237387488...|\n|      0|       0.0|[0.91166691833234...|\n|      0|       0.0|[0.89779287791520...|\n|      0|       0.0|[0.90955831349140...|\n|      0|       0.0|[0.83952406620263...|\n|      0|       0.0|[0.87633797334720...|\n|      1|       0.0|[0.70659397948078...|\n|      0|       0.0|[0.83338543652976...|\n|      0|       0.0|[0.88280204065791...|\n|      0|       0.0|[0.82665688325614...|\n|      0|       0.0|[0.84758751536107...|\n|      0|       0.0|[0.89742340713220...|\n|      0|       0.0|[0.90965005804922...|\n|      0|       0.0|[0.89476582615290...|\n|      1|       0.0|[0.83780144123522...|\n|      0|       0.0|[0.83083523990668...|\n|      0|       0.0|[0.85522051005572...|\n|      0|       0.0|[0.90428057667988...|\n|      1|       0.0|[0.86289174147460...|\n|      0|       0.0|[0.83415868124847...|\n|      0|       0.0|[0.90780177910639...|\n|      0|       0.0|[0.89938633430161...|\n|      1|       0.0|[0.83572229553176...|\n|      0|       0.0|[0.90654612044397...|\n|      0|       0.0|[0.82455171446912...|\n|      1|       0.0|[0.67214183956629...|\n|      0|       0.0|[0.82394267757034...|\n|      1|       1.0|[0.35886303263693...|\n|      0|       0.0|[0.82394267757034...|\n|      0|       0.0|[0.89523392273235...|\n|      0|       0.0|[0.89500586597340...|\n|      1|       0.0|[0.84398944809852...|\n|      0|       0.0|[0.89554535138842...|\n|      1|       1.0|[0.44332229697881...|\n|      0|       0.0|[0.90174630261947...|\n|      0|       0.0|[0.82132531472514...|\n|      0|       0.0|[0.89455330827582...|\n|      0|       0.0|[0.91627004908648...|\n|      0|       0.0|[0.83739737098425...|\n|      0|       0.0|[0.83739737098425...|\n|      1|       0.0|[0.70545327492644...|\n|      0|       0.0|[0.90234064822049...|\n|      0|       0.0|[0.91613962127276...|\n|      0|       0.0|[0.91089118477627...|\n|      0|       0.0|[0.80477826757901...|\n|      0|       0.0|[0.89939338082541...|\n|      0|       0.0|[0.85921753991303...|\n|      0|       0.0|[0.91517063009607...|\n|      0|       0.0|[0.87083743643132...|\n|      0|       0.0|[0.89978611665840...|\n|      0|       0.0|[0.55900188195329...|\n|      0|       0.0|[0.90971473894335...|\n|      0|       0.0|[0.86730321027750...|\n|      0|       0.0|[0.90258080635363...|\n|      0|       0.0|[0.91410206278678...|\n|      0|       0.0|[0.54584877796829...|\n|      0|       0.0|[0.88865414249506...|\n|      0|       0.0|[0.86730321027750...|\n|      0|       0.0|[0.90898794877269...|\n|      0|       0.0|[0.88804591990622...|\n|      0|       0.0|[0.82831678585394...|\n|      0|       0.0|[0.89545358031829...|\n|      0|       0.0|[0.90675510713771...|\n|      0|       0.0|[0.82500028297007...|\n|      0|       0.0|[0.89784150902503...|\n|      0|       0.0|[0.89425608927914...|\n|      0|       0.0|[0.89742543463882...|\n|      0|       0.0|[0.85306686274733...|\n|      0|       0.0|[0.82038810046127...|\n|      0|       0.0|[0.89345683863766...|\n|      1|       0.0|[0.63668254030730...|\n|      0|       0.0|[0.79163972613328...|\n|      0|       0.0|[0.80453276444146...|\n|      0|       0.0|[0.85036538307881...|\n|      0|       0.0|[0.84880117726788...|\n|      0|       0.0|[0.91389360729378...|\n|      1|       0.0|[0.73520355617602...|\n|      0|       0.0|[0.83538640941412...|\n|      0|       0.0|[0.80913541984480...|\n|      0|       0.0|[0.90148841508895...|\n|      0|       0.0|[0.83093681459543...|\n|      0|       0.0|[0.85349077526546...|\n|      0|       0.0|[0.83396261851939...|\n|      0|       0.0|[0.89890421470209...|\n|      0|       0.0|[0.83193916859322...|\n|      0|       0.0|[0.89923875915956...|\n|      0|       0.0|[0.85198243332992...|\n|      0|       0.0|[0.83376482433111...|\n|      0|       0.0|[0.84953272801671...|\n|      0|       0.0|[0.82625544491326...|\n|      0|       0.0|[0.82686448181205...|\n|      0|       0.0|[0.83176249664791...|\n|      0|       0.0|[0.89786832166393...|\n|      0|       0.0|[0.84757163688183...|\n|      0|       0.0|[0.88826617841402...|\n|      0|       0.0|[0.88793163395655...|\n|      0|       0.0|[0.82546005545696...|\n|      0|       0.0|[0.85566621052022...|\n|      0|       0.0|[0.87083626327749...|\n|      0|       0.0|[0.88974390472640...|\n+-------+----------+--------------------+\nonly showing top 150 rows\n"
     ]
    }
   ],
   "source": [
    "# Show the predictions\n",
    "predictions.select(\"Delayed\", \"prediction\", \"probability\").show(150)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "800370ba-679a-408d-8b54-cb708f092a79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+--------------+\n|Delayed|prediction|delayed_status|\n+-------+----------+--------------+\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      1|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      1|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      1|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      1|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      1|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      1|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      1|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      1|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      1|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      1|       1.0|       Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      1|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      1|       1.0|       Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      1|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      1|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      1|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n|      0|       0.0|   Not Delayed|\n+-------+----------+--------------+\nonly showing top 150 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Adding a column to classify predictions based on the probability\n",
    "df_final = predictions.withColumn(\n",
    "    \"delayed_status\", \n",
    "    F.when(F.col(\"prediction\") == 1.0, \"Delayed\")\n",
    "    .otherwise(\"Not Delayed\")\n",
    ")\n",
    "\n",
    "# Displaying the final predictions with the status\n",
    "df_final.select(\"Delayed\", \"prediction\", \"delayed_status\").show(150)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7874dc47-55e9-4061-ac3a-83d4d333a938",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Group1_analysis",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}